#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
QwenåŒä¹‰è¯æœç´¢å™¨ V3ç‰ˆæœ¬ - ä¼˜åŒ–ç‰ˆ
å…ˆæŒ‰éŸµæ¯ç­›é€‰ï¼Œå†è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå¤§å¹…æå‡æŸ¥è¯¢é€Ÿåº¦
"""

import ast
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
import re
from qwen_embedding_client import QwenEmbeddingClient
from pinyin_tools import (
    get_word_pinyin, filter_words_by_character_finals, 
    filter_words_by_advanced_criteria, get_available_initials, 
    get_available_finals, get_available_tones, get_available_strokes, 
    get_available_radicals
)
from vocabulary_preprocessor import VocabularyPreprocessor
import json
import os

class QwenSynonymSearcherV3:
    """QwenåŒä¹‰è¯æœç´¢å™¨V3 - å…ˆç­›é€‰å†è®¡ç®—ç‰ˆæœ¬"""
    
    def __init__(self, qwen_client=None):
        """åˆå§‹åŒ–æœç´¢å™¨"""
        print("ğŸš€ åˆå§‹åŒ–QwenSynonymSearcherV3...")
        
        # åˆå§‹åŒ–Qwenå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        if qwen_client:
            self.qwen_client = qwen_client
            self.qwen_available = qwen_client.available
        else:
            try:
                self.qwen_client = QwenEmbeddingClient()
                if self.qwen_client.available:
                    print("âœ… Qwen embeddingæœåŠ¡è¿æ¥æˆåŠŸ")
                    self.qwen_available = True
                else:
                    print("âš ï¸ Qwen embeddingæœåŠ¡ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼")
                    self.qwen_available = False
            except Exception as e:
                print(f"âš ï¸ Qwenå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("ğŸ”§ åˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼ï¼ˆä»…æ”¯æŒç­›é€‰åŠŸèƒ½ï¼‰")
                self.qwen_client = None
                self.qwen_available = False
        
        # åˆå§‹åŒ–è¯åº“é¢„å¤„ç†å™¨
        print("ğŸ”„ åˆå§‹åŒ–è¯åº“é¢„å¤„ç†å™¨...")
        self.vocab_preprocessor = VocabularyPreprocessor()
        
        # åŠ è½½é¢„å¤„ç†çš„è¯åº“æ•°æ®
        if self.vocab_preprocessor.preprocess_vocabulary():
            self.candidate_words = self.vocab_preprocessor.get_all_words()
            print(f"âœ… å·²åŠ è½½é¢„å¤„ç†è¯åº“: {len(self.candidate_words)} ä¸ªè¯æ±‡")
        else:
            # å¤‡ç”¨æ–¹æ³•åŠ è½½å€™é€‰è¯åº“
            print("âš ï¸ é¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½å€™é€‰è¯...")
            self.candidate_words = self._load_candidate_words()
            if self.candidate_words:
                print(f"ğŸ“š å·²åŠ è½½å€™é€‰è¯åº“: {len(self.candidate_words)} ä¸ªè¯æ±‡")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å€™é€‰è¯åº“æ•°æ®")
        
        # å°è¯•åŠ è½½å¤§è¯åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.large_vocabulary = self._load_large_vocabulary()
        if self.large_vocabulary:
            print(f"ğŸ“– å·²åŠ è½½å¤§è¯åº“: {len(self.large_vocabulary)} ä¸ªè¯æ±‡")
    
    def get_word_pinyin_fast(self, word: str) -> List[str]:
        """
        å¿«é€Ÿè·å–è¯æ±‡æ‹¼éŸ³ï¼ˆä½¿ç”¨é¢„å¤„ç†çš„æ•°æ®ï¼‰
        
        Args:
            word: è¯æ±‡
            
        Returns:
            æ‹¼éŸ³åˆ—è¡¨
        """
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            pinyins = self.vocab_preprocessor.get_word_pinyin(word)
            if pinyins:
                return pinyins
        
        # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨åŸå§‹å‡½æ•°
        return get_word_pinyin(word)
    
    def _load_candidate_words(self) -> List[str]:
        """åŠ è½½åŸºç¡€å€™é€‰è¯åº“ - ä»…ä»å®é™…æ•°æ®æºåŠ è½½ï¼Œä¸ç¼–é€ """
        try:
            # å°è¯•ä»ci.jsonåŠ è½½è¯æ±‡
            import os
            import json
            
            ci_path = os.path.join(os.path.dirname(__file__), "ci.json")
            if os.path.exists(ci_path):
                with open(ci_path, 'r', encoding='utf-8') as f:
                    ci_data = json.load(f)
                    words = [item.get('ci', '').strip() for item in ci_data if item.get('ci') and len(item.get('ci', '').strip()) >= 2]
                    return words[:500]  # é™åˆ¶ä¸ºå‰500ä¸ªä½œä¸ºåŸºç¡€è¯åº“
            
            print("âš ï¸ æœªæ‰¾åˆ°ci.jsonè¯åº“æ–‡ä»¶")
            return []
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å€™é€‰è¯åº“å¤±è´¥: {e}")
            return []
    
    def _load_large_vocabulary(self) -> Optional[List[str]]:
        """å°è¯•åŠ è½½å¤§è¯åº“ï¼ˆci.jsonï¼‰"""
        try:
            ci_path = "ci.json"
            if os.path.exists(ci_path):
                with open(ci_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                words = []
                seen = set()
                for item in data:
                    word = item.get('ci', '').strip()
                    if word and len(word) >= 2 and word not in seen:
                        words.append(word)
                        seen.add(word)
                return words
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å¤§è¯åº“å¤±è´¥: {e}")
        return None
    
    def _is_chinese(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
        chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
        return bool(chinese_pattern.search(text))
    
    def search_synonyms(self, word: str, k: int = 10, character_finals: Optional[List[str]] = None, 
                       character_filters: Optional[List[Dict[str, Any]]] = None, 
                       min_length: Optional[int] = None, max_length: Optional[int] = None) -> Tuple[List[str], List[float], str]:
        """
        åŒä¹‰è¯æŸ¥è¯¢ - ç»Ÿä¸€å¤„ç†æ¶æ„
        æ–°æ¶æ„ï¼š1.ç»Ÿä¸€ç­›é€‰é€»è¾‘è·å–å€™é€‰è¯ â†’ 2.æœ€åæ£€æŸ¥æ˜¯å¦æœ‰æŸ¥è¯¢è¯å†³å®šæ’åºæ–¹å¼
        
        Args:
            word: æŸ¥è¯¢è¯æ±‡ï¼ˆå¯ä»¥ä¸ºç©ºï¼Œè¡¨ç¤ºçº¯ç­›é€‰ï¼‰
            k: è¿”å›ç»“æœæ•°é‡
            character_finals: æ¯ä¸ªå­—ç¬¦ä½ç½®çš„éŸµæ¯è¦æ±‚ï¼Œç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ— è¦æ±‚ï¼ˆç®€å•æ¨¡å¼ï¼‰
            character_filters: é«˜çº§ç­›é€‰æ¡ä»¶ï¼Œæ¯ä¸ªä½ç½®çš„è¯¦ç»†ç­›é€‰å‚æ•°ï¼ˆé«˜çº§æ¨¡å¼ï¼‰
            min_length: æœ€å°è¯é•¿é™åˆ¶ï¼ˆå¯é€‰ï¼‰
            max_length: æœ€å¤§è¯é•¿é™åˆ¶ï¼ˆå¯é€‰ï¼‰
        """
        try:
            # åˆ¤æ–­æ˜¯å¦æœ‰æŸ¥è¯¢è¯
            has_query_word = bool(word and word.strip())
            
            if has_query_word:
                word = word.strip()
                if not self._is_chinese(word):
                    return [], [], "âš ï¸ è¾“å…¥çš„è¯æ±‡ä¸æ˜¯ä¸­æ–‡"
                print(f"ğŸ” å¼€å§‹æŸ¥è¯¢: {word}")
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç­›é€‰æ¡ä»¶ï¼ˆéŸµæ¯ã€é«˜çº§ç­›é€‰ã€é•¿åº¦ç­›é€‰ï¼‰
                has_any_filter = False
                if character_finals and any(f for f in character_finals):
                    has_any_filter = True
                if character_filters and any(f for f in character_filters):
                    has_any_filter = True
                if min_length is not None or max_length is not None:
                    has_any_filter = True
                    
                if not has_any_filter:
                    return [], [], "âŒ è¯·è¾“å…¥æŸ¥è¯¢è¯æ±‡æˆ–è®¾ç½®ç­›é€‰æ¡ä»¶"
                print("ğŸ” å¯åŠ¨çº¯ç­›é€‰æ¨¡å¼ï¼ˆæ— æŸ¥è¯¢è¯ï¼‰")
            
            # æ­¥éª¤1: ç»Ÿä¸€ç­›é€‰å€™é€‰è¯ï¼ˆæ— è®ºæ˜¯å¦æœ‰æŸ¥è¯¢è¯ï¼Œç­›é€‰é€»è¾‘å®Œå…¨ç›¸åŒï¼‰
            print("âš¡ ç»Ÿä¸€ç­›é€‰å€™é€‰è¯...")
            eligible_words = self._unified_filter_candidates(word, character_finals, character_filters, min_length, max_length)
            
            if not eligible_words:
                if character_finals and any(f for f in character_finals):
                    return [], [], "ğŸµ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆéŸµæ¯æ¡ä»¶çš„è¯æ±‡"
                elif character_filters:
                    return [], [], "ğŸ¯ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆç­›é€‰æ¡ä»¶çš„è¯æ±‡"
                else:
                    return [], [], "âŒ æ²¡æœ‰æ‰¾åˆ°å€™é€‰è¯æ±‡"
            
            print(f"âœ… ç­›é€‰å‡º {len(eligible_words)} ä¸ªå€™é€‰è¯")
            
            # æ­¥éª¤2: æœ€åæ£€æŸ¥æ˜¯å¦æœ‰æŸ¥è¯¢è¯å†³å®šæ’åºæ–¹å¼
            if has_query_word and self.qwen_available:
                # æœ‰æŸ¥è¯¢è¯ä¸”æœåŠ¡å¯ç”¨ï¼šè®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
                print("ğŸ§  è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº...")
                synonyms, similarities = self._compute_similarities_and_sort(word, eligible_words, k)
                
                if not synonyms:
                    return [], [], "âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥"
                
                result_msg = self._build_unified_result_message(word, synonyms, similarities, character_finals, character_filters, is_sorted=True, min_length=min_length, max_length=max_length)
                return synonyms, similarities, result_msg
            
            else:
                # æ— æŸ¥è¯¢è¯æˆ–æœåŠ¡ä¸å¯ç”¨ï¼šç›´æ¥è¿”å›ç­›é€‰ç»“æœ
                if has_query_word:
                    print("âš ï¸ QwenæœåŠ¡ä¸å¯ç”¨ï¼Œè¿”å›ç­›é€‰ç»“æœï¼ˆæ— æ’åºï¼‰")
                else:
                    print("ğŸ“‹ è¿”å›ç­›é€‰ç»“æœï¼ˆçº¯ç­›é€‰æ¨¡å¼ï¼‰")
                
                final_results = eligible_words[:k]
                zero_similarities = [0.0] * len(final_results)
                result_msg = self._build_unified_result_message(word, final_results, zero_similarities, character_finals, character_filters, is_sorted=False, min_length=min_length, max_length=max_length)
                return final_results, zero_similarities, result_msg
            
        except Exception as e:
            error_msg = f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"
            print(error_msg)
            return [], [], error_msg
    
    def _unified_filter_candidates(self, query_word: str, character_finals: Optional[List[str]] = None, 
                                 character_filters: Optional[List[Dict[str, Any]]] = None,
                                 min_length: Optional[int] = None, max_length: Optional[int] = None) -> List[str]:
        """
        ç»Ÿä¸€ç­›é€‰å€™é€‰è¯çš„å‡½æ•° - æ— è®ºæ˜¯å¦æœ‰æŸ¥è¯¢è¯ï¼Œç­›é€‰é€»è¾‘å®Œå…¨ç›¸åŒ
        """
        # å…ˆè·å–åŸºç¡€å€™é€‰è¯æ± 
        if character_filters:
            # é«˜çº§ç­›é€‰æ¨¡å¼
            print("ğŸ¯ é«˜çº§ç­›é€‰æ¨¡å¼")
            eligible_words = self._unified_advanced_filter(character_filters)
        elif character_finals and any(f for f in character_finals):
            # ç®€å•éŸµæ¯ç­›é€‰æ¨¡å¼
            print("ğŸµ éŸµæ¯ç­›é€‰æ¨¡å¼")
            eligible_words = self._unified_finals_filter(character_finals)
        else:
            # åŸºç¡€æ¨¡å¼ï¼šä»æ‰€æœ‰è¯æ±‡å¼€å§‹
            print("ğŸ“š åŸºç¡€ç­›é€‰æ¨¡å¼")
            eligible_words = self._get_base_vocabulary()
        
        # åº”ç”¨é•¿åº¦ç­›é€‰
        if min_length is not None or max_length is not None:
            eligible_words = self._filter_by_length(eligible_words, min_length, max_length)
            print(f"ğŸ“ é•¿åº¦ç­›é€‰å: {len(eligible_words)} ä¸ªå€™é€‰è¯")
        
        # ç§»é™¤æŸ¥è¯¢è¯æœ¬èº«ï¼ˆå¦‚æœæœ‰ï¼‰
        if query_word and query_word.strip():
            eligible_words = [w for w in eligible_words if w != query_word]
            print(f"ğŸš« ç§»é™¤æŸ¥è¯¢è¯å: {len(eligible_words)} ä¸ªå€™é€‰è¯")
        
        return eligible_words
    
    def _get_base_vocabulary(self) -> List[str]:
        """è·å–åŸºç¡€è¯æ±‡åº“"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            # ä½¿ç”¨é¢„å¤„ç†è¯åº“ï¼ˆ2-10å­—ï¼‰
            candidates = self.vocab_preprocessor.filter_words_by_length(2, 10)
            print(f"âš¡ ä½¿ç”¨é¢„å¤„ç†è¯åº“: {len(candidates)} ä¸ªè¯æ±‡")
            return candidates
        elif self.large_vocabulary and len(self.large_vocabulary) > 1000:
            # ä½¿ç”¨å¤§è¯åº“ï¼Œè¿‡æ»¤å•å­—
            candidates = [w for w in self.large_vocabulary if len(w) >= 2]
            print(f"ğŸ“– ä½¿ç”¨å¤§è¯åº“: {len(candidates)} ä¸ªè¯æ±‡")
            return candidates
        else:
            # ä½¿ç”¨åŸºç¡€è¯åº“ï¼Œè¿‡æ»¤å•å­—
            candidates = [w for w in self.candidate_words if len(w) >= 2]
            print(f"ğŸ“š ä½¿ç”¨åŸºç¡€è¯åº“: {len(candidates)} ä¸ªè¯æ±‡")
            return candidates
    
    def _unified_finals_filter(self, character_finals: List[str]) -> List[str]:
        """ç»Ÿä¸€éŸµæ¯ç­›é€‰é€»è¾‘"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            return self._efficient_filter_by_finals_unified(character_finals)
        else:
            # å¤‡ç”¨æ–¹æ³•ï¼šä»åŸºç¡€è¯æ±‡å¼€å§‹ç­›é€‰
            candidates = self._get_base_vocabulary()
            filtered_words = filter_words_by_character_finals(candidates, character_finals)
            print(f"ğŸ“š å¤‡ç”¨éŸµæ¯ç­›é€‰å®Œæˆ: {len(filtered_words)} ä¸ªå€™é€‰è¯")
            return filtered_words
    
    def _unified_advanced_filter(self, character_filters: List[Dict[str, Any]]) -> List[str]:
        """ç»Ÿä¸€é«˜çº§ç­›é€‰é€»è¾‘"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            return self._efficient_advanced_filter_unified(character_filters)
        else:
            # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨åˆ†å±‚ç­›é€‰
            candidates = self._get_base_vocabulary()
            filtered_words = self._apply_layered_filtering(candidates, character_filters)
            print(f"ğŸ“š å¤‡ç”¨é«˜çº§ç­›é€‰å®Œæˆ: {len(filtered_words)} ä¸ªå€™é€‰è¯")
            return filtered_words
    
    def _efficient_filter_by_finals_unified(self, character_finals: List[str]) -> List[str]:
        """
        ç»Ÿä¸€é«˜æ•ˆéŸµæ¯ç­›é€‰ç®—æ³•ï¼š
        1. äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯
        2. é€è¯éªŒè¯æ‰€æœ‰æ¡ä»¶
        """
        print(f"   ç›®æ ‡éŸµæ¯æ¡ä»¶: {character_finals}")
        
        # æ­¥éª¤1: ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯é›†åˆ
        initial_candidates = self._get_candidates_by_binary_search(character_finals)
        
        if not initial_candidates:
            print("   äºŒåˆ†æŸ¥æ‰¾æ— ç»“æœ")
            return []
        
        print(f"   äºŒåˆ†æŸ¥æ‰¾ç»“æœ: {len(initial_candidates)} ä¸ªå€™é€‰è¯")
        
        # æ­¥éª¤2: é€è¯éªŒè¯æ‰€æœ‰æ¡ä»¶
        verified_candidates = self._verify_candidates_conditions(initial_candidates, character_finals)
        
        print(f"   éªŒè¯å®Œæˆ: {len(verified_candidates)} ä¸ªç¬¦åˆæ¡ä»¶çš„è¯")
        return verified_candidates
    
    def _efficient_advanced_filter_unified(self, character_filters: List[Dict[str, Any]]) -> List[str]:
        """
        ç»Ÿä¸€é«˜æ•ˆçš„é«˜çº§ç­›é€‰ç®—æ³•ï¼š
        1. å¤šçº§ç´¢å¼•å®šä½å€™é€‰è¯ï¼ˆå£°æ¯/éŸµæ¯ > ç¬”ç”»æ•° > å£°è°ƒ > éƒ¨é¦–ï¼‰
        2. æš´åŠ›æšä¸¾éªŒè¯æ‰€æœ‰æ¡ä»¶
        """
        print(f"   é«˜çº§ç­›é€‰æ¡ä»¶: {len(character_filters)} ä¸ªå­—ç¬¦ä½ç½®")
        
        # æ­¥éª¤1: ä½¿ç”¨å¤šçº§ç´¢å¼•è·å–åˆå§‹å€™é€‰è¯é›†åˆ
        initial_candidates = self._get_candidates_by_advanced_binary_search(character_filters)
        
        if not initial_candidates:
            print("   ç´¢å¼•æŸ¥æ‰¾æ— ç»“æœ")
            return []
        
        print(f"   ç´¢å¼•å®šä½ç»“æœ: {len(initial_candidates)} ä¸ªå€™é€‰è¯")
        
        # æ­¥éª¤2: æš´åŠ›æšä¸¾éªŒè¯æ‰€æœ‰æ¡ä»¶
        print("   å¼€å§‹æš´åŠ›æšä¸¾éªŒè¯...")
        verified_candidates = self._brute_force_verify_all_conditions(initial_candidates, character_filters)
        
        print(f"   æš´åŠ›éªŒè¯å®Œæˆ: {len(verified_candidates)} ä¸ªç¬¦åˆæ¡ä»¶çš„è¯")
        return verified_candidates
    
    def _compute_similarities_and_sort(self, query_word: str, candidates: List[str], k: int) -> Tuple[List[str], List[float]]:
        """
        è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åºï¼Œè¿”å›å‰kä¸ªç»“æœ
        ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥è®¡ç®—å¹¶æ’åºï¼Œé¿å…ä¸­é—´æ­¥éª¤
        """
        
        if not candidates:
            return [], []
        
        try:
            print(f"ğŸ§  è®¡ç®— {len(candidates)} ä¸ªå€™é€‰è¯çš„ç›¸ä¼¼åº¦...")
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = self._compute_batch_similarities_optimized(query_word, candidates)
            
            if not similarities:
                print("âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥")
                return [], []
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # è¿”å›å‰kä¸ªç»“æœ
            top_k = similarities[:k]
            synonyms = [pair[0] for pair in top_k]
            sim_scores = [pair[1] for pair in top_k]
            
            print(f"âœ… æ’åºå®Œæˆï¼Œè¿”å›å‰{len(synonyms)}ä¸ªç»“æœ")
            return synonyms, sim_scores
            
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return [], []
    
    def _compute_batch_similarities_optimized(self, query_word: str, candidates: List[str]) -> List[Tuple[str, float]]:
        """ä¼˜åŒ–çš„æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—"""
        try:
            # å‡†å¤‡ç¼–ç è¯æ±‡åˆ—è¡¨ï¼ˆæŸ¥è¯¢è¯ + å€™é€‰è¯ï¼‰
            words_to_encode = [query_word] + candidates
            
            # åˆ†æ‰¹ç¼–ç ä»¥é¿å…è¶…è¿‡APIé™åˆ¶
            batch_size = 25  # ä¿å®ˆçš„æ‰¹æ¬¡å¤§å°
            all_embeddings = []
            
            total_batches = (len(words_to_encode) + batch_size - 1) // batch_size
            
            for i in range(0, len(words_to_encode), batch_size):
                batch = words_to_encode[i:i + batch_size]
                print(f"   ç¼–ç æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches}: {len(batch)} ä¸ªè¯")
                
                batch_embeddings = self.qwen_client.encode(batch)
                
                if batch_embeddings is None:
                    print(f"âŒ æ‰¹æ¬¡ç¼–ç å¤±è´¥")
                    continue
                
                all_embeddings.extend(batch_embeddings)
            
            if len(all_embeddings) != len(words_to_encode):
                print(f"âš ï¸ ç¼–ç æ•°é‡ä¸åŒ¹é…: {len(all_embeddings)} vs {len(words_to_encode)}")
                return []
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            query_embedding = all_embeddings[0]
            candidate_embeddings = all_embeddings[1:]
            
            similarities = []
            for i, candidate in enumerate(candidates):
                if i < len(candidate_embeddings):
                    candidate_embedding = candidate_embeddings[i]
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_embedding, candidate_embedding) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding)
                    )
                    similarities.append((candidate, float(similarity)))
            
            return similarities
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return []
    
    def search_synonyms_advanced(
        self, 
        word: str, 
        k: int = 10,
        character_filters: Optional[List[Dict[str, Any]]] = None
    ) -> Tuple[List[str], List[float], str]:
        """
        é«˜çº§åŒä¹‰è¯æŸ¥è¯¢ - æ”¯æŒå£°æ¯ã€éŸµæ¯ã€å£°è°ƒã€ç¬”ç”»ã€éƒ¨é¦–ç­‰å¤šé‡ç­›é€‰
        
        Args:
            word: æŸ¥è¯¢è¯æ±‡ï¼ˆå¯ä»¥ä¸ºç©ºï¼Œè¡¨ç¤ºçº¯ç­›é€‰æ¨¡å¼ï¼‰
            k: è¿”å›ç»“æœæ•°é‡
            character_filters: æ¯ä¸ªå­—ç¬¦ä½ç½®çš„ç­›é€‰æ¡ä»¶åˆ—è¡¨
                æ ¼å¼: [
                    {  # ç¬¬ä¸€ä¸ªå­—çš„æ¡ä»¶
                        'initial': 'å£°æ¯',
                        'final': 'éŸµæ¯', 
                        'tone': 'å£°è°ƒ',
                        'stroke_count': ç¬”ç”»æ•°,
                        'radical': 'éƒ¨é¦–',
                        'contains_stroke': 'ç¬”ç”»åç§°',
                        'stroke_position': ç¬”ç”»ä½ç½®,  # å•ä¸ªç¬”ç”»ä½ç½®ï¼ˆå…¼å®¹æ€§ï¼‰
                        'stroke_positions': {ä½ç½®1: 'ç¬”ç”»1', ä½ç½®2: 'ç¬”ç”»2', ...}  # å¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶
                    },
                    {  # ç¬¬äºŒä¸ªå­—çš„æ¡ä»¶
                        ...
                    }
                ]
        
        Returns:
            (åŒä¹‰è¯åˆ—è¡¨, ç›¸ä¼¼åº¦åˆ—è¡¨, ç»“æœæ¶ˆæ¯)
        """
        try:
            # ğŸš€ æ–°å¢ï¼šæ”¯æŒçº¯ç­›é€‰æ¨¡å¼ï¼ˆæŸ¥è¯¢è¯ä¸ºç©ºï¼‰
            if not word or not word.strip():
                if character_filters:
                    print("ğŸ” å¯åŠ¨çº¯ç­›é€‰æ¨¡å¼ï¼ˆæ— éœ€embeddingæœåŠ¡ï¼‰")
                    return self._pure_filter_search(character_filters, k)
                else:
                    return [], [], "âŒ è¯·è¾“å…¥æŸ¥è¯¢è¯æ±‡æˆ–è®¾ç½®ç­›é€‰æ¡ä»¶"
            
            word = word.strip()
            if not self._is_chinese(word):
                return [], [], "âš ï¸ è¾“å…¥çš„è¯æ±‡ä¸æ˜¯ä¸­æ–‡"
            
            print(f"ğŸ” å¼€å§‹é«˜çº§æŸ¥è¯¢: {word}")
            
            # æ­¥éª¤1: æŒ‰é«˜çº§æ¡ä»¶ç­›é€‰å€™é€‰è¯
            print("ğŸ“ æ­¥éª¤1: æŒ‰é«˜çº§æ¡ä»¶ç­›é€‰å€™é€‰è¯...")
            eligible_words = self._filter_candidates_by_advanced_criteria(word, character_filters)
            
            if not eligible_words:
                # ğŸš€ æ–°æœºåˆ¶ï¼šæ²¡æœ‰ç¬¦åˆç­›é€‰æ¡ä»¶çš„è¯æ—¶ï¼Œç›´æ¥è¿”å›å‰kä¸ªå€™é€‰è¯ï¼ˆä¸è¿›è¡Œè¯­ä¹‰æ’åºï¼‰
                print("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆç­›é€‰æ¡ä»¶çš„è¯æ±‡ï¼Œè¿”å›å‰kä¸ªå€™é€‰è¯")
                candidates = self._get_candidate_pool(word)
                # ç§»é™¤æŸ¥è¯¢è¯æœ¬èº«ï¼Œå–å‰kä¸ª
                top_candidates = [w for w in candidates if w != word][:k]
                
                # è¿”å›ç©ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè¡¨ç¤ºæœªè¿›è¡Œè¯­ä¹‰è®¡ç®—ï¼‰
                zero_similarities = [0.0] * len(top_candidates)
                result_msg = f"ğŸ¯ æœªæ‰¾åˆ°ç¬¦åˆç­›é€‰æ¡ä»¶çš„è¯æ±‡ï¼Œæ˜¾ç¤ºå‰{len(top_candidates)}ä¸ªå€™é€‰è¯ï¼ˆæœªæ’åºï¼‰"
                
                return top_candidates, zero_similarities, result_msg
            
            print(f"âœ… ç­›é€‰å‡º {len(eligible_words)} ä¸ªå€™é€‰è¯")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
            if not self.qwen_available:
                print("âš ï¸ QwenæœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡è¯­ä¹‰æ’åº")
                # ç›´æ¥è¿”å›ç­›é€‰ç»“æœï¼Œä¸è¿›è¡Œè¯­ä¹‰æ’åº
                final_results = eligible_words[:k]
                zero_similarities = [0.0] * len(final_results)
                result_msg = self._build_advanced_result_message(word, final_results, zero_similarities, character_filters)
                return final_results, zero_similarities, result_msg
            
            # æ­¥éª¤2: å¯¹ç­›é€‰åçš„è¯æ±‡è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
            print("ğŸ§  æ­¥éª¤2: è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº...")
            synonyms, similarities = self._compute_similarities(word, eligible_words, k)
            
            if not synonyms:
                return [], [], "âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥"
            
            # æ„å»ºç»“æœæ¶ˆæ¯
            result_msg = self._build_advanced_result_message(word, synonyms, similarities, character_filters)
            
            return synonyms, similarities, result_msg
            
        except Exception as e:
            error_msg = f"âŒ é«˜çº§æŸ¥è¯¢å¤±è´¥: {str(e)}"
            print(error_msg)
            return [], [], error_msg
    
    def _get_candidate_pool(self, query_word: str) -> List[str]:
        """è·å–å€™é€‰è¯æ± ï¼ˆç”¨äºç›¸ä¼¼åº¦è®¡ç®—ï¼‰"""
        
        # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†çš„è¯æ±‡åº“
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            candidates = self.vocab_preprocessor.get_all_words()
            print(f"âš¡ ä½¿ç”¨é¢„å¤„ç†è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯")
        elif self.large_vocabulary and len(self.large_vocabulary) > 1000:
            # ä½¿ç”¨å¤§è¯åº“
            candidates = self.large_vocabulary
            print(f"ğŸ“– ä½¿ç”¨å¤§è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯")
        else:
            candidates = self.candidate_words
            print(f"ğŸ“š ä½¿ç”¨åŸºç¡€è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯")
        
        # è¿‡æ»¤æ‰æŸ¥è¯¢è¯æœ¬èº«å’Œå•å­—è¯
        filtered = [w for w in candidates if w != query_word and len(w) >= 2]
        return filtered
    
    def _filter_candidates_by_finals(self, query_word: str, character_finals: Optional[List[str]]) -> List[str]:
        """æŒ‰éŸµæ¯æ¡ä»¶ç­›é€‰å€™é€‰è¯ï¼ˆæ—§ç‰ˆæœ¬ - å·²è¢«ç»Ÿä¸€æ¶æ„æ›¿ä»£ï¼‰"""
        print("âš ï¸ è­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨æ—§ç‰ˆç­›é€‰å‡½æ•°ï¼Œå»ºè®®ä½¿ç”¨æ–°çš„ç»Ÿä¸€æ¶æ„")
        return self._unified_finals_filter(character_finals)
    
    def _filter_candidates_by_length_only(self, query_word: str) -> List[str]:
        """çº¯é•¿åº¦ç­›é€‰ï¼ˆæ—§ç‰ˆæœ¬ - å·²è¢«ç»Ÿä¸€æ¶æ„æ›¿ä»£ï¼‰"""
        print("âš ï¸ è­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨æ—§ç‰ˆç­›é€‰å‡½æ•°ï¼Œå»ºè®®ä½¿ç”¨æ–°çš„ç»Ÿä¸€æ¶æ„")
        return self._get_base_vocabulary()
    
    def _efficient_filter_by_finals(self, query_word: str, character_finals: List[str]) -> List[str]:
        """
        é«˜æ•ˆçš„éŸµæ¯ç­›é€‰ç®—æ³•ï¼š
        1. äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯
        2. é€è¯éªŒè¯æ‰€æœ‰æ¡ä»¶
        """
        print(f"   ç›®æ ‡éŸµæ¯æ¡ä»¶: {character_finals}")
        
        # æ­¥éª¤1: ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯é›†åˆ
        initial_candidates = self._get_candidates_by_binary_search(character_finals)
        
        if not initial_candidates:
            print("   äºŒåˆ†æŸ¥æ‰¾æ— ç»“æœ")
            return []
        
        print(f"   äºŒåˆ†æŸ¥æ‰¾ç»“æœ: {len(initial_candidates)} ä¸ªå€™é€‰è¯")
        
        # æ­¥éª¤2: é€è¯éªŒè¯æ‰€æœ‰æ¡ä»¶
        verified_candidates = self._verify_candidates_conditions(initial_candidates, character_finals)
        
        # ç§»é™¤æŸ¥è¯¢è¯æœ¬èº«ï¼ˆå¦‚æœæœ‰æŸ¥è¯¢è¯ï¼‰
        if query_word and query_word.strip():
            verified_candidates = [w for w in verified_candidates if w != query_word]
        
        print(f"   éªŒè¯å®Œæˆ: {len(verified_candidates)} ä¸ªç¬¦åˆæ¡ä»¶çš„è¯")
        return verified_candidates
    
    def _get_candidates_by_binary_search(self, character_finals: List[str]) -> List[str]:
        """ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯é›†åˆ"""
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªéç©ºçš„éŸµæ¯æ¡ä»¶ç”¨äºäºŒåˆ†æŸ¥æ‰¾
        primary_final = None
        primary_position = -1
        
        for i, final in enumerate(character_finals):
            if final and final.strip():
                primary_final = final.strip()
                primary_position = i
                break
        
        if not primary_final:
            # æ²¡æœ‰éŸµæ¯æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰è¯æ±‡
            return self.vocab_preprocessor.get_all_words()
        
        print(f"   ä½¿ç”¨ç¬¬{primary_position+1}ä¸ªå­—çš„éŸµæ¯'{primary_final}'è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾")
        
        # ä½¿ç”¨é¢„å¤„ç†å™¨çš„éŸµæ¯ç´¢å¼•å¿«é€Ÿè·å–å€™é€‰è¯
        candidates = self.vocab_preprocessor.filter_words_by_final_fast(primary_final)
        
        # å¦‚æœå€™é€‰è¯å¤ªå°‘ï¼Œå°è¯•ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ‰©å±•
        if len(candidates) < 100:
            # å°è¯•æŒ‰æ‹¼éŸ³å‰ç¼€æœç´¢ï¼ˆç”¨äºå¤„ç†å¤åˆéŸµæ¯ï¼‰
            prefix_candidates = self.vocab_preprocessor.binary_search_by_pinyin_prefix(primary_final)
            # åˆå¹¶ç»“æœå¹¶å»é‡
            all_candidates = list(set(candidates + prefix_candidates))
            print(f"   æ‰©å±•æœç´¢: {len(candidates)} + {len(prefix_candidates)} = {len(all_candidates)} ä¸ªå€™é€‰è¯")
            return all_candidates
        
        return candidates
    
    def _verify_candidates_conditions(self, candidates: List[str], character_finals: List[str]) -> List[str]:
        """é€è¯éªŒè¯å€™é€‰è¯æ˜¯å¦æ»¡è¶³æ‰€æœ‰éŸµæ¯æ¡ä»¶"""
        
        verified = []
        target_length = len(character_finals)
        
        # å¯¼å…¥ pinyin_tools ä»¥è·å–å®æ—¶æ‹¼éŸ³
        from pinyin_tools import get_word_finals
        
        for word in candidates:
            # æ£€æŸ¥è¯é•¿æ˜¯å¦åŒ¹é…
            if len(word) != target_length:
                continue
            
            # ä½¿ç”¨ pinyin_tools è·å–å®æ—¶éŸµæ¯
            try:
                actual_finals = get_word_finals(word)
                
                # è¿‡æ»¤æ‰éæ‹¼éŸ³æ•°æ®ï¼ˆå¦‚"èª¬"ç­‰æ³¨é‡Šï¼‰
                clean_finals = []
                for final in actual_finals:
                    # åªä¿ç•™åŒ…å«æ‹¼éŸ³å­—ç¬¦çš„éŸµæ¯
                    if final and all(c.isalpha() or c in 'Ã¼É¡' for c in final):
                        clean_finals.append(final)
                
                # æ£€æŸ¥æ¸…ç†åçš„éŸµæ¯æ•°é‡æ˜¯å¦åŒ¹é…
                if len(clean_finals) != target_length:
                    continue
                
                # éªŒè¯æ¯ä¸ªä½ç½®çš„éŸµæ¯
                matches = True
                for i, (required_final, actual_final) in enumerate(zip(character_finals, clean_finals)):
                    if required_final and required_final.strip():
                        required = required_final.strip()
                        
                        # æ ‡å‡†åŒ–éŸµæ¯æ¯”è¾ƒï¼šå¤„ç†Unicodeå­—ç¬¦É¡å’Œueâ†”veè½¬æ¢
                        def normalize_final(final):
                            """æ ‡å‡†åŒ–éŸµæ¯æ ¼å¼ï¼Œæ”¯æŒueâ†”veåŒå‘è½¬æ¢"""
                            if not final:
                                return ''
                            # å¤„ç†Unicode É¡å­—ç¬¦
                            normalized = final.replace('É¡', 'g')
                            # å¤„ç†ueâ†”veè½¬æ¢ï¼šéƒ½ç»Ÿä¸€ä¸ºueè¿›è¡Œæ¯”è¾ƒ
                            if normalized == 've':
                                normalized = 'ue'
                            return normalized
                        
                        if normalize_final(actual_final) != normalize_final(required):
                            matches = False
                            break
                
                if matches:
                    verified.append(word)
                        
            except Exception:
                # å¦‚æœæ‹¼éŸ³è·å–å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªè¯
                continue
        
        return verified
    
    # ========== æ—§ç­›é€‰å‡½æ•°ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰==========
    # æ³¨æ„ï¼šä»¥ä¸‹å‡½æ•°å·²è¢«ç»Ÿä¸€æ¶æ„æ›¿ä»£ï¼Œä½†ä¿ç•™ä»¥é˜²å…¶ä»–ä»£ç è°ƒç”¨
    
    def _filter_candidates_by_advanced_criteria(self, query_word: str, character_filters: Optional[List[Dict[str, Any]]]) -> List[str]:
        """æŒ‰é«˜çº§æ¡ä»¶ç­›é€‰å€™é€‰è¯ï¼ˆæ—§ç‰ˆæœ¬ - å·²è¢«ç»Ÿä¸€æ¶æ„æ›¿ä»£ï¼‰"""
        print("âš ï¸ è­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨æ—§ç‰ˆç­›é€‰å‡½æ•°ï¼Œå»ºè®®ä½¿ç”¨æ–°çš„ç»Ÿä¸€æ¶æ„")
        return self._unified_advanced_filter(character_filters)
    
    def _efficient_advanced_filter(self, query_word: str, character_filters: List[Dict[str, Any]]) -> List[str]:
        """
        é«˜æ•ˆçš„é«˜çº§ç­›é€‰ç®—æ³•ï¼š
        1. å¤šçº§ç´¢å¼•å®šä½å€™é€‰è¯ï¼ˆå£°æ¯/éŸµæ¯ > ç¬”ç”»æ•° > å£°è°ƒ > éƒ¨é¦–ï¼‰
        2. æš´åŠ›æšä¸¾éªŒè¯æ‰€æœ‰æ¡ä»¶
        """
        print(f"   é«˜çº§ç­›é€‰æ¡ä»¶: {len(character_filters)} ä¸ªå­—ç¬¦ä½ç½®")
        
        # æ­¥éª¤1: ä½¿ç”¨å¤šçº§ç´¢å¼•è·å–åˆå§‹å€™é€‰è¯é›†åˆ
        initial_candidates = self._get_candidates_by_advanced_binary_search(character_filters)
        
        if not initial_candidates:
            print("   ç´¢å¼•æŸ¥æ‰¾æ— ç»“æœ")
            return []
        
        print(f"   ç´¢å¼•å®šä½ç»“æœ: {len(initial_candidates)} ä¸ªå€™é€‰è¯")
        
        # æ­¥éª¤2: æš´åŠ›æšä¸¾éªŒè¯æ‰€æœ‰æ¡ä»¶
        print("   å¼€å§‹æš´åŠ›æšä¸¾éªŒè¯...")
        verified_candidates = self._brute_force_verify_all_conditions(initial_candidates, character_filters)
        
        # ç§»é™¤æŸ¥è¯¢è¯æœ¬èº«
        verified_candidates = [w for w in verified_candidates if w != query_word]
        
        print(f"   æš´åŠ›éªŒè¯å®Œæˆ: {len(verified_candidates)} ä¸ªç¬¦åˆæ¡ä»¶çš„è¯")
        return verified_candidates
    
    def _get_candidates_by_advanced_binary_search(self, character_filters: List[Dict[str, Any]]) -> List[str]:
        """
        ä½¿ç”¨å¤šçº§äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯é›†åˆ
        æŒ‰ç…§è¦æ±‚çš„ä¼˜å…ˆçº§ï¼šå£°æ¯/éŸµæ¯ > ç¬”ç”»æ•° > å£°è°ƒ > éƒ¨é¦–
        """
        
        # å¯»æ‰¾æœ€ä¼˜çš„äºŒåˆ†æŸ¥æ‰¾æ¡ä»¶ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        best_filter = None
        best_position = -1
        best_type = None
        best_priority = 0
        
        for i, filters in enumerate(character_filters):
            if not filters:
                continue
            
            # ä¼˜å…ˆçº§1ï¼šå£°æ¯/éŸµæ¯ï¼ˆæ‹¼éŸ³ä¿¡æ¯æœ€å…·é€‰æ‹©æ€§ï¼‰
            if filters.get('final'):
                best_filter = filters['final']
                best_position = i
                best_type = 'final'
                best_priority = 5
                break  # éŸµæ¯ä¼˜å…ˆçº§æœ€é«˜ï¼Œç›´æ¥ä½¿ç”¨
            elif filters.get('initial') and best_priority < 4:
                best_filter = filters['initial']
                best_position = i
                best_type = 'initial'
                best_priority = 4
            
            # ä¼˜å…ˆçº§2ï¼šç¬”ç”»æ•°ï¼ˆå­—ç¬¦ç»“æ„ä¿¡æ¯ï¼‰
            elif filters.get('stroke_count') and best_priority < 3:
                best_filter = filters['stroke_count']
                best_position = i
                best_type = 'stroke_count'
                best_priority = 3
            
            # ä¼˜å…ˆçº§3ï¼šå£°è°ƒï¼ˆæ‹¼éŸ³è¾…åŠ©ä¿¡æ¯ï¼‰
            elif filters.get('tone') and best_priority < 2:
                best_filter = filters['tone']
                best_position = i
                best_type = 'tone'
                best_priority = 2
            
            # ä¼˜å…ˆçº§4ï¼šéƒ¨é¦–ï¼ˆå­—ç¬¦åˆ†ç±»ä¿¡æ¯ï¼‰
            elif filters.get('radical') and best_priority < 1:
                best_filter = filters['radical']
                best_position = i
                best_type = 'radical'
                best_priority = 1
        
        if not best_filter:
            # æ²¡æœ‰å¯ç”¨çš„äºŒåˆ†æŸ¥æ‰¾æ¡ä»¶ï¼Œè¿”å›æŒ‰æ‹¼éŸ³æ’åºçš„å…¨è¯åº“
            print("   æ— äºŒåˆ†æŸ¥æ‰¾æ¡ä»¶ï¼Œä½¿ç”¨æ‹¼éŸ³æ’åºè¯åº“")
            return self._get_pinyin_sorted_vocabulary()
        
        print(f"   ä½¿ç”¨ç¬¬{best_position+1}ä¸ªå­—çš„{best_type}'{best_filter}'è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾")
        
        # æŒ‰ç…§è¦æ±‚çš„é¢„å¤„ç†é¡ºåºä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾
        if best_type == 'final' or best_type == 'initial':
            # ä½¿ç”¨æ‹¼éŸ³æ’åºè¯åº“è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾
            candidates = self._binary_search_by_pinyin(best_type, best_filter, best_position)
        elif best_type == 'stroke_count':
            # ä½¿ç”¨ç¬”ç”»æ•°æ’åºè¯åº“è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾
            candidates = self._binary_search_by_stroke_count(best_filter, best_position)
        elif best_type == 'tone':
            # ä½¿ç”¨å£°è°ƒä¿¡æ¯è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾ï¼ˆå›é€€åˆ°æ‹¼éŸ³æ’åºï¼‰
            candidates = self._binary_search_by_tone(best_filter, best_position)
        elif best_type == 'radical':
            # ä½¿ç”¨éƒ¨é¦–æ’åºè¯åº“è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾
            candidates = self._binary_search_by_radical(best_filter, best_position)
        else:
            candidates = self._get_pinyin_sorted_vocabulary()
        
        print(f"   äºŒåˆ†æŸ¥æ‰¾ç»“æœ: {len(candidates)} ä¸ªå€™é€‰è¯")
        return candidates
    
    def _get_pinyin_sorted_vocabulary(self) -> List[str]:
        """è·å–æŒ‰æ‹¼éŸ³æ’åºçš„è¯åº“"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            # ä½¿ç”¨é¢„å¤„ç†å™¨è·å–æŒ‰æ‹¼éŸ³æ’åºçš„è¯æ±‡
            return self.vocab_preprocessor.get_words_sorted_by_pinyin()
        else:
            # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨åŸºç¡€è¯åº“
            return self.candidate_words
    
    def _get_stroke_sorted_vocabulary(self) -> List[str]:
        """è·å–æŒ‰ç¬”ç”»æ•°æ’åºçš„è¯åº“"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            # ä½¿ç”¨é¢„å¤„ç†å™¨è·å–æŒ‰ç¬”ç”»æ•°æ’åºçš„è¯æ±‡
            return self.vocab_preprocessor.get_words_sorted_by_stroke_count()
        else:
            # å¤‡ç”¨æ–¹æ³•ï¼šæŒ‰ç¬”ç”»æ•°æ’åºåŸºç¡€è¯åº“
            return sorted(self.candidate_words, key=lambda word: self._get_word_total_strokes(word))
    
    def _get_radical_sorted_vocabulary(self) -> List[str]:
        """è·å–æŒ‰éƒ¨é¦–æ’åºçš„è¯åº“"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            # ä½¿ç”¨é¢„å¤„ç†å™¨è·å–æŒ‰éƒ¨é¦–æ’åºçš„è¯æ±‡
            return self.vocab_preprocessor.get_words_sorted_by_radical()
        else:
            # å¤‡ç”¨æ–¹æ³•ï¼šæŒ‰é¦–å­—éƒ¨é¦–æ’åºåŸºç¡€è¯åº“
            return sorted(self.candidate_words, key=lambda word: self._get_word_first_radical(word))
    
    def _binary_search_by_pinyin(self, search_type: str, target_value: str, position: int) -> List[str]:
        """
        åœ¨æ‹¼éŸ³æ’åºçš„è¯åº“ä¸­äºŒåˆ†æŸ¥æ‰¾
        search_type: 'initial' æˆ– 'final'
        target_value: ç›®æ ‡å£°æ¯æˆ–éŸµæ¯
        position: å­—ç¬¦ä½ç½®ï¼ˆ0å¼€å§‹ï¼‰
        """
        vocabulary = self._get_pinyin_sorted_vocabulary()
        
        # ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ‰¾åˆ°åŒ¹é…çš„è¯æ±‡èŒƒå›´
        matching_words = []
        
        for word in vocabulary:
            if len(word) <= position:
                continue
                
            # è·å–æŒ‡å®šä½ç½®å­—ç¬¦çš„æ‹¼éŸ³ä¿¡æ¯
            char_pinyin = self._get_character_pinyin_at_position(word, position)
            if not char_pinyin:
                continue
            
            # æå–å£°æ¯å’ŒéŸµæ¯
            initial, final = self.vocab_preprocessor._extract_initial_final(char_pinyin) if hasattr(self, 'vocab_preprocessor') else self._extract_initial_final_simple(char_pinyin)
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡å€¼
            if search_type == 'initial':
                # å¤„ç†å£°æ¯åŒ¹é…ï¼ˆåŒ…æ‹¬å­—ç¬¦è½¬æ¢ï¼‰
                search_initial = target_value
                if search_initial == 'g':
                    search_initial = 'É¡'  # Unicode U+0261
                if initial == search_initial:
                    matching_words.append(word)
            elif search_type == 'final':
                # æ”¯æŒéŸµæ¯åŒå‘åŒ¹é…ï¼ˆue <-> veï¼‰
                if self._final_matches(final, target_value):
                    matching_words.append(word)
        
        return matching_words
    
    def _binary_search_by_stroke_count(self, target_stroke_count: int, position: int) -> List[str]:
        """
        åœ¨ç¬”ç”»æ•°æ’åºçš„è¯åº“ä¸­äºŒåˆ†æŸ¥æ‰¾
        target_stroke_count: ç›®æ ‡ç¬”ç”»æ•°
        position: å­—ç¬¦ä½ç½®ï¼ˆ0å¼€å§‹ï¼‰
        """
        print(f"âš¡ ä½¿ç”¨çœŸæ­£çš„äºŒåˆ†æœç´¢æŸ¥æ‰¾ç¬¬{position+1}ä¸ªå­—æœ‰{target_stroke_count}ç”»çš„è¯æ±‡")
        
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            # ä½¿ç”¨é¢„å¤„ç†å™¨çš„å¿«é€ŸäºŒåˆ†æœç´¢æ–¹æ³•
            if position == 0:
                # ç¬¬ä¸€ä¸ªå­—ï¼Œä½¿ç”¨æœ€å¿«çš„ç´¢å¼•æŸ¥æ‰¾
                result = self.vocab_preprocessor.filter_words_by_stroke_count_fast(target_stroke_count)
                print(f"   ç´¢å¼•æŸ¥æ‰¾ç»“æœ: {len(result)} ä¸ªè¯æ±‡")
                return result
            else:
                # éç¬¬ä¸€ä¸ªå­—ï¼Œä½¿ç”¨å­—ç¬¦çº§äºŒåˆ†æœç´¢
                result = self.vocab_preprocessor.binary_search_by_character_stroke_count(position, target_stroke_count)
                print(f"   å­—ç¬¦çº§æœç´¢ç»“æœ: {len(result)} ä¸ªè¯æ±‡")
                return result
        
        # å›é€€åˆ°åŸå§‹æ…¢é€Ÿæ–¹æ³•
        print("   è­¦å‘Šï¼šå›é€€åˆ°æ…¢é€Ÿéå†æ–¹æ³•")
        vocabulary = self._get_stroke_sorted_vocabulary()
        
        matching_words = []
        for word in vocabulary:
            if len(word) <= position:
                continue
                
            char = word[position]
            char_info = self._get_character_stroke_count(char)
            
            if char_info == target_stroke_count:
                matching_words.append(word)
        
        return matching_words
    
    def _binary_search_by_tone(self, target_tone: str, position: int) -> List[str]:
        """
        æŒ‰å£°è°ƒè¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾ï¼ˆå›é€€åˆ°æ‹¼éŸ³æ’åºè¯åº“ï¼‰
        target_tone: ç›®æ ‡å£°è°ƒ
        position: å­—ç¬¦ä½ç½®ï¼ˆ0å¼€å§‹ï¼‰
        """
        vocabulary = self._get_pinyin_sorted_vocabulary()
        
        matching_words = []
        for word in vocabulary:
            if len(word) <= position:
                continue
                
            char_pinyin = self._get_character_pinyin_at_position(word, position)
            if not char_pinyin:
                continue
            
            tone = self._extract_tone(char_pinyin)
            if tone == target_tone:
                matching_words.append(word)
        
        return matching_words
    
    def _binary_search_by_radical(self, target_radical: str, position: int) -> List[str]:
        """
        åœ¨éƒ¨é¦–æ’åºçš„è¯åº“ä¸­äºŒåˆ†æŸ¥æ‰¾
        target_radical: ç›®æ ‡éƒ¨é¦–
        position: å­—ç¬¦ä½ç½®ï¼ˆ0å¼€å§‹ï¼‰
        """
        vocabulary = self._get_radical_sorted_vocabulary()
        
        matching_words = []
        for word in vocabulary:
            if len(word) <= position:
                continue
                
            char = word[position]
            char_radical = self._get_character_radical(char)
            
            if char_radical == target_radical:
                matching_words.append(word)
        
        return matching_words
    
    def _get_character_pinyin_at_position(self, word: str, position: int) -> str:
        """è·å–è¯æ±‡ä¸­æŒ‡å®šä½ç½®å­—ç¬¦çš„æ‹¼éŸ³"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            pinyins = self.vocab_preprocessor.get_word_pinyin(word)
            if pinyins and len(pinyins) > 0:
                # å¤„ç†æ‹¼éŸ³æ ¼å¼
                pinyin_string = pinyins[0]
                if isinstance(pinyin_string, str) and pinyin_string.startswith('[') and pinyin_string.endswith(']'):
                    try:
                        import ast
                        pinyin_list = ast.literal_eval(pinyin_string)
                        if isinstance(pinyin_list, list) and position < len(pinyin_list):
                            return pinyin_list[position]
                    except:
                        pinyin_string = pinyin_string.strip("[]'\"").replace("'", "").replace('"', '')
                
                pinyin_parts = pinyin_string.split()
                if position < len(pinyin_parts):
                    return pinyin_parts[position]
        
        # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨pinyin_tools
        from pinyin_tools import get_word_pinyin
        pinyins = get_word_pinyin(word)
        if pinyins and position < len(pinyins):
            return pinyins[position]
        
        return ""
    
    def _extract_initial_final_simple(self, pinyin: str) -> Tuple[str, str]:
        """ç®€å•çš„å£°æ¯éŸµæ¯æå–ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        if not pinyin:
            return "", ""
        
        # ç®€åŒ–çš„å£°æ¯åˆ—è¡¨
        initials = ['zh', 'ch', 'sh', 'b', 'p', 'm', 'f', 'd', 't', 'n', 'l', 'g', 'k', 'h', 'j', 'q', 'x', 'z', 'c', 's', 'r', 'y', 'w']
        
        # æ‰¾åˆ°æœ€é•¿åŒ¹é…çš„å£°æ¯
        initial = ""
        for i in sorted(initials, key=len, reverse=True):
            if pinyin.startswith(i):
                initial = i
                break
        
        # å‰©ä½™éƒ¨åˆ†ä¸ºéŸµæ¯
        final = pinyin[len(initial):]
        
        # å»é™¤å£°è°ƒç¬¦å·å¾—åˆ°éŸµæ¯
        import re
        final = re.sub(r'[1-5\u0300-\u036f]', '', final)
        
        return initial, final
    
    def _final_matches(self, actual_final: str, target_final: str) -> bool:
        """æ£€æŸ¥éŸµæ¯æ˜¯å¦åŒ¹é…ï¼ˆæ”¯æŒue <-> veåŒå‘è½¬æ¢ï¼‰"""
        if not actual_final or not target_final:
            return actual_final == target_final
        
        # æ ‡å‡†åŒ–éŸµæ¯
        def normalize_final(f):
            if not f:
                return ''
            # å¤„ç†Unicode É¡å­—ç¬¦
            normalized = f.replace('É¡', 'g')
            return normalized
        
        actual_norm = normalize_final(actual_final)
        target_norm = normalize_final(target_final)
        
        # ç›´æ¥åŒ¹é…
        if actual_norm == target_norm:
            return True
        
        # ue <-> ve åŒå‘åŒ¹é…
        if (actual_norm == 'ue' and target_norm == 've') or (actual_norm == 've' and target_norm == 'ue'):
            return True
        
        return False
    
    def _get_character_stroke_count(self, char: str) -> int:
        """è·å–å­—ç¬¦çš„ç¬”ç”»æ•°"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            char_info = self.vocab_preprocessor.get_character_info(char)
            if char_info:
                return char_info.get('strokes', 0)  # ä¿®æ­£ï¼šstroke -> strokes
        
        # å¤‡ç”¨æ–¹æ³•ï¼šè¿”å›é»˜è®¤å€¼
        return 0
    
    def _get_character_radical(self, char: str) -> str:
        """è·å–å­—ç¬¦çš„éƒ¨é¦–"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            char_info = self.vocab_preprocessor.get_character_info(char)
            if char_info:
                return char_info.get('radical', '')
        
        # å¤‡ç”¨æ–¹æ³•ï¼šè¿”å›ç©ºå­—ç¬¦ä¸²
        return ""
    
    def _get_word_total_strokes(self, word: str) -> int:
        """è·å–è¯æ±‡çš„æ€»ç¬”ç”»æ•°ï¼ˆç”¨äºæ’åºï¼‰"""
        total = 0
        for char in word:
            total += self._get_character_stroke_count(char)
        return total
    
    def _get_word_first_radical(self, word: str) -> str:
        """è·å–è¯æ±‡é¦–å­—çš„éƒ¨é¦–ï¼ˆç”¨äºæ’åºï¼‰"""
        if word:
            return self._get_character_radical(word[0])
        return ""
    
    def _brute_force_verify_all_conditions(self, candidates: List[str], character_filters: List[Dict[str, Any]]) -> List[str]:
        """
        æš´åŠ›æšä¸¾éªŒè¯æ‰€æœ‰æ¡ä»¶
        å¯¹æ¯ä¸ªå€™é€‰è¯é€ä¸€æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ‰€æœ‰ä½ç½®çš„æ‰€æœ‰æ¡ä»¶
        """
        verified = []
        
        # è®¡ç®—å®é™…æœ‰æ•ˆçš„ç­›é€‰æ¡ä»¶æ•°é‡
        max_position = 0
        for i, f in enumerate(character_filters):
            if f:
                max_position = i + 1
        
        if max_position == 0:
            return candidates  # æ²¡æœ‰ç­›é€‰æ¡ä»¶
        
        total_candidates = len(candidates)
        verified_count = 0
        
        for idx, word in enumerate(candidates):
            # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯å¤„ç†100ä¸ªè¯æ˜¾ç¤ºä¸€æ¬¡ï¼‰
            if idx % 100 == 0 and idx > 0:
                print(f"   éªŒè¯è¿›åº¦: {idx}/{total_candidates} ({verified_count} ä¸ªé€šè¿‡)")
            
            # è¯é•¿å¿…é¡»è‡³å°‘åŒ…å«æœ‰æ¡ä»¶çš„ä½ç½®
            if len(word) < max_position:
                continue
            
            # æš´åŠ›éªŒè¯æ¯ä¸ªå­—ç¬¦ä½ç½®çš„æ‰€æœ‰æ¡ä»¶
            if self._verify_word_all_conditions(word, character_filters):
                verified.append(word)
                verified_count += 1
        
        print(f"   éªŒè¯å®Œæˆ: {total_candidates} ä¸ªå€™é€‰è¯ -> {verified_count} ä¸ªé€šè¿‡")
        return verified
    
    def _verify_word_all_conditions(self, word: str, character_filters: List[Dict[str, Any]]) -> bool:
        """
        éªŒè¯å•ä¸ªè¯æ±‡æ˜¯å¦æ»¡è¶³æ‰€æœ‰æ¡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        """
        
        # è·å–è¯æ±‡æ‹¼éŸ³ï¼ˆä½¿ç”¨é¢„å¤„ç†å™¨ç¼“å­˜ï¼‰
        pinyins = self.vocab_preprocessor.get_word_pinyin(word)
        if not pinyins:
            return False
        
        # å¤„ç†æ‹¼éŸ³æ ¼å¼
        pinyin_string = pinyins[0]
        if isinstance(pinyin_string, str) and pinyin_string.startswith('[') and pinyin_string.endswith(']'):
            try:
                import ast
                pinyin_list = ast.literal_eval(pinyin_string)
                if isinstance(pinyin_list, list) and len(pinyin_list) > 0:
                    pinyin_string = ' '.join(pinyin_list)
            except:
                pinyin_string = pinyin_string.strip("[]'\"").replace("'", "").replace('"', '')
        
        pinyin_parts = pinyin_string.split()
        
        # é€å­—ç¬¦éªŒè¯æ‰€æœ‰æ¡ä»¶
        for i, filters in enumerate(character_filters):
            if not filters:  # ç©ºæ¡ä»¶ï¼Œè·³è¿‡
                continue
                
            # æ£€æŸ¥è¯æ±‡æ˜¯å¦æœ‰è¶³å¤Ÿçš„å­—ç¬¦
            if i >= len(word) or i >= len(pinyin_parts):
                return False
            
            char = word[i]
            char_pinyin = pinyin_parts[i]
            
            # éªŒè¯æ‹¼éŸ³ç›¸å…³æ¡ä»¶ï¼ˆå£°æ¯ã€éŸµæ¯ã€å£°è°ƒï¼‰
            if not self._verify_pinyin_conditions(char_pinyin, filters):
                return False
            
            # éªŒè¯å­—ç¬¦ç›¸å…³æ¡ä»¶ï¼ˆç¬”ç”»ã€éƒ¨é¦–ç­‰ï¼‰
            if not self._verify_character_conditions(char, filters):
                return False
        
        return True
    
    def _filter_by_length(self, candidates: List[str], min_length: Optional[int], max_length: Optional[int]) -> List[str]:
        """æŒ‰è¯æ±‡é•¿åº¦ç­›é€‰å€™é€‰è¯"""
        if min_length is None and max_length is None:
            return candidates
        
        filtered = []
        for word in candidates:
            word_length = len(word)
            # æ£€æŸ¥æœ€å°é•¿åº¦
            if min_length is not None and word_length < min_length:
                continue
            # æ£€æŸ¥æœ€å¤§é•¿åº¦
            if max_length is not None and word_length > max_length:
                continue
            filtered.append(word)
        
        return filtered
    
    def _verify_word_conditions_legacy(self, word: str, character_filters: List[Dict[str, Any]]) -> bool:
        """éªŒè¯å•ä¸ªè¯æ±‡æ˜¯å¦æ»¡è¶³æ‰€æœ‰æ¡ä»¶"""
        
        # è·å–è¯æ±‡æ‹¼éŸ³
        pinyins = self.vocab_preprocessor.get_word_pinyin(word)
        if not pinyins:
            return False
        
        # å¤„ç†æ‹¼éŸ³æ ¼å¼ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²è¡¨ç¤ºçš„åˆ—è¡¨ï¼Œéœ€è¦è§£æ
        pinyin_string = pinyins[0]
        if isinstance(pinyin_string, str) and pinyin_string.startswith('[') and pinyin_string.endswith(']'):
            # è§£æå­—ç¬¦ä¸²è¡¨ç¤ºçš„åˆ—è¡¨ï¼Œå¦‚"['xuÃ©']" -> "xuÃ©"
            import ast
            try:
                import ast
                pinyin_list = ast.literal_eval(pinyin_string)
                if isinstance(pinyin_list, list) and len(pinyin_list) > 0:
                    pinyin_string = ' '.join(pinyin_list)
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•çš„å­—ç¬¦ä¸²å¤„ç†
                pinyin_string = pinyin_string.strip("[]'\"").replace("'", "").replace('"', '')
        
        pinyin_parts = pinyin_string.split()
        
        # é€å­—ç¬¦éªŒè¯ï¼ˆåªéªŒè¯æœ‰æ¡ä»¶çš„ä½ç½®ï¼‰
        for i, filters in enumerate(character_filters):
            if not filters:  # ç©ºæ¡ä»¶ï¼Œè·³è¿‡
                continue
                
            # æ£€æŸ¥è¯æ±‡æ˜¯å¦æœ‰è¶³å¤Ÿçš„å­—ç¬¦
            if i >= len(word) or i >= len(pinyin_parts):
                return False
            
            char = word[i]
            char_pinyin = pinyin_parts[i]
            
            # éªŒè¯æ‹¼éŸ³ç›¸å…³æ¡ä»¶ï¼ˆå£°æ¯ã€éŸµæ¯ã€å£°è°ƒï¼‰
            if not self._verify_pinyin_conditions(char_pinyin, filters):
                return False
            
            # éªŒè¯å­—ç¬¦ç›¸å…³æ¡ä»¶ï¼ˆç¬”ç”»ã€éƒ¨é¦–ç­‰ï¼‰
            if not self._verify_character_conditions(char, filters):
                return False
        
        return True
    
    def _verify_pinyin_conditions(self, char_pinyin: str, filters: Dict[str, Any]) -> bool:
        """éªŒè¯æ‹¼éŸ³ç›¸å…³æ¡ä»¶ï¼ˆç©ºå€¼è¡¨ç¤ºæ— é™åˆ¶ï¼‰"""
        
        # æå–å£°æ¯å’ŒéŸµæ¯
        initial, final = self.vocab_preprocessor._extract_initial_final(char_pinyin)
        
        # éªŒè¯å£°æ¯ï¼ˆç©ºå€¼æˆ–Noneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_initial = filters.get('initial')
        if required_initial and required_initial.strip():  # æœ‰å…·ä½“è¦æ±‚
            # å¤„ç†å£°æ¯å­—ç¬¦è½¬æ¢ï¼ˆg -> É¡ï¼‰
            search_initial = required_initial.strip()
            if search_initial == 'g':
                search_initial = 'É¡'  # Unicode U+0261
            if initial != search_initial:
                return False
        
        # éªŒè¯éŸµæ¯ï¼ˆç©ºå€¼æˆ–Noneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_final = filters.get('final')
        if required_final and required_final.strip():  # æœ‰å…·ä½“è¦æ±‚
            # æ”¯æŒéŸµæ¯æ ‡å‡†åŒ–æ¯”è¾ƒ
            def normalize_final(f):
                if not f:
                    return ''
                # å¤„ç†Unicode É¡å­—ç¬¦
                normalized = f.replace('É¡', 'g')
                # å¤„ç†ue <-> veè½¬æ¢ï¼šç»Ÿä¸€ä¸ºueæ ‡å‡†
                if normalized == 've':
                    normalized = 'ue'
                return normalized
            
            actual_normalized = normalize_final(final)
            required_normalized = normalize_final(required_final.strip())
            
            # æ”¯æŒåŒå‘åŒ¹é…ï¼šueå¯ä»¥åŒ¹é…veï¼Œveä¹Ÿå¯ä»¥åŒ¹é…ue
            if actual_normalized != required_normalized and not (
                (actual_normalized == 'ue' and required_normalized == 've') or
                (actual_normalized == 've' and required_normalized == 'ue')
            ):
                return False
        
        # éªŒè¯å£°è°ƒï¼ˆéœ€è¦ä»åŸå§‹æ‹¼éŸ³æå–ï¼‰
        required_tone = filters.get('tone')
        if required_tone and required_tone.strip():  # æœ‰å…·ä½“è¦æ±‚
            # ç®€å•çš„å£°è°ƒæå–ï¼ˆåŸºäºéŸ³è°ƒç¬¦å·ï¼‰
            tone = self._extract_tone(char_pinyin)
            if tone != filters['tone']:
                return False
        
        return True
    
    def _verify_character_conditions(self, char: str, filters: Dict[str, Any]) -> bool:
        """éªŒè¯å­—ç¬¦ç›¸å…³æ¡ä»¶ï¼ˆç¬”ç”»ã€éƒ¨é¦–ç­‰ï¼Œç©ºå€¼è¡¨ç¤ºæ— é™åˆ¶ï¼‰"""
        
        # è·å–å­—ç¬¦ä¿¡æ¯
        char_info = self.vocab_preprocessor.get_character_info(char)
        if not char_info:
            # å¦‚æœæ²¡æœ‰å­—ç¬¦ä¿¡æ¯ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¿…é¡»éªŒè¯çš„æ¡ä»¶
            has_required_conditions = any([
                filters.get('stroke_count'),
                filters.get('radical') and filters.get('radical').strip(),
                filters.get('contains_stroke') and filters.get('contains_stroke').strip(),
                filters.get('stroke_positions') and isinstance(filters.get('stroke_positions'), dict)
            ])
            return not has_required_conditions
        
        # éªŒè¯ç¬”ç”»æ•°ï¼ˆ0æˆ–Noneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_stroke = filters.get('stroke_count')
        if required_stroke and required_stroke > 0:  # æœ‰å…·ä½“è¦æ±‚
            actual_stroke = char_info.get('strokes', 0)  # ä¿®æ­£ï¼šstroke -> strokes
            if actual_stroke != required_stroke:
                return False
        
        # éªŒè¯éƒ¨é¦–ï¼ˆç©ºå€¼æˆ–Noneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_radical = filters.get('radical')
        if required_radical and required_radical.strip():  # æœ‰å…·ä½“è¦æ±‚
            actual_radical = char_info.get('radical', '')
            if actual_radical != required_radical.strip():
                return False
        
        # éªŒè¯ç¬”ç”»ç›¸å…³æ¡ä»¶ï¼ˆç©ºå€¼è¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_stroke_char = filters.get('contains_stroke')
        stroke_positions = filters.get('stroke_positions')
        
        # å¦‚æœæœ‰ä»»ä½•ç¬”ç”»ç›¸å…³æ¡ä»¶ï¼Œéƒ½éœ€è¦éªŒè¯
        if (required_stroke_char and required_stroke_char.strip()) or (stroke_positions and isinstance(stroke_positions, dict)):
            order_simple = char_info.get('order_simple', [])
            if not self._verify_stroke_conditions(order_simple, filters):
                return False
        
        return True
    
    def _extract_tone(self, pinyin) -> str:
        """ä»æ‹¼éŸ³ä¸­æå–å£°è°ƒ"""
        # å£°è°ƒç¬¦å·æ˜ å°„
        tone_map = {
            'Ä': '1', 'Ã¡': '2', 'Ç': '3', 'Ã ': '4',
            'Ä“': '1', 'Ã©': '2', 'Ä›': '3', 'Ã¨': '4',
            'Ä«': '1', 'Ã­': '2', 'Ç': '3', 'Ã¬': '4',
            'Å': '1', 'Ã³': '2', 'Ç’': '3', 'Ã²': '4',
            'Å«': '1', 'Ãº': '2', 'Ç”': '3', 'Ã¹': '4',
            'Ç–': '1', 'Ç˜': '2', 'Çš': '3', 'Çœ': '4',
            'Å„': '2', 'Åˆ': '3', 'Ç¹': '4',
            'á¸¿': '2', 'Åˆ': '3', 'Ç¹': '4'
        }
        
        # å¤„ç†ä¸åŒçš„æ‹¼éŸ³æ ¼å¼
        if isinstance(pinyin, list) and pinyin:
            clean_pinyin = pinyin[0]  # å–ç¬¬ä¸€ä¸ªæ‹¼éŸ³
        elif isinstance(pinyin, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²è¡¨ç¤ºçš„åˆ—è¡¨æ ¼å¼ï¼Œå¦‚ "['xuÃ©']"
            if pinyin.startswith('[') and pinyin.endswith(']'):
                # è§£æå­—ç¬¦ä¸²è¡¨ç¤ºçš„åˆ—è¡¨
                import ast
                try:
                    parsed = ast.literal_eval(pinyin)
                    if isinstance(parsed, list) and parsed:
                        clean_pinyin = parsed[0]
                    else:
                        clean_pinyin = pinyin.strip("[]'\"")
                except:
                    clean_pinyin = pinyin.strip("[]'\"")
            else:
                clean_pinyin = pinyin.strip()
        else:
            return '5'
        
        # æ¸…ç†æ‹¼éŸ³å­—ç¬¦ä¸²
        clean_pinyin = str(clean_pinyin).strip()
        
        for char in clean_pinyin:
            if char in tone_map:
                return tone_map[char]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å£°è°ƒç¬¦å·ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•°å­—å£°è°ƒï¼ˆå¦‚xue2ï¼‰
        import re
        tone_match = re.search(r'(\d)$', clean_pinyin)
        if tone_match:
            return tone_match.group(1)
        
        return '5'  # è½»å£°æˆ–æ— å£°è°ƒ
    
    def _verify_stroke_conditions(self, order_simple: List[str], filters: Dict[str, Any]) -> bool:
        """éªŒè¯ç¬”ç”»ç›¸å…³æ¡ä»¶ï¼Œæ”¯æŒå¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶"""
        
        # é˜²æŠ¤ï¼šç¡®ä¿ order_simple ä¸ä¸º None
        if order_simple is None:
            order_simple = []
        
        contains_stroke = filters.get('contains_stroke')
        stroke_position = filters.get('stroke_position')
        stroke_positions = filters.get('stroke_positions')  # æ–°å¢ï¼šå¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶
        
        # ä¼˜å…ˆå¤„ç†å¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶
        if stroke_positions and isinstance(stroke_positions, dict):
            for position, expected_stroke in stroke_positions.items():
                # è½¬æ¢ä¸º0ç´¢å¼•
                pos = int(position) - 1
                if pos < 0 or pos >= len(order_simple) or order_simple[pos] != expected_stroke:
                    return False
        
        # å…¼å®¹æ€§å¤„ç†ï¼šå•ä¸ªç¬”ç”»ä½ç½®é™åˆ¶
        if contains_stroke:
            if stroke_position:
                # éªŒè¯ç‰¹å®šä½ç½®çš„ç¬”ç”»
                pos = int(stroke_position) - 1  # è½¬æ¢ä¸º0ç´¢å¼•
                if pos < 0 or pos >= len(order_simple) or order_simple[pos] != contains_stroke:
                    return False
            else:
                # éªŒè¯æ˜¯å¦åŒ…å«æŸç§ç¬”ç”»
                if contains_stroke not in order_simple:
                    return False
        
        return True
    
    def _apply_layered_filtering(self, candidates: List[str], character_filters: List[Dict[str, Any]]) -> List[str]:
        """åˆ†å±‚ç­›é€‰ç­–ç•¥ï¼šé¢„ç­›é€‰ â†’ é€æ­¥ç²¾åŒ– â†’ æœ€ç»ˆä¼˜åŒ–"""
        
        # ç¬¬ä¸€å±‚ï¼šé¢„ç­›é€‰ - ä½¿ç”¨æœ€ä¸¥æ ¼çš„æ¡ä»¶å¿«é€Ÿè¿‡æ»¤
        print("ğŸ” ç¬¬ä¸€å±‚ï¼šé¢„ç­›é€‰é˜¶æ®µ...")
        filtered_candidates = self._pre_filter_candidates(candidates, character_filters)
        
        if not filtered_candidates:
            print("âŒ é¢„ç­›é€‰åæ— ç»“æœ")
            return []
        
        print(f"   é¢„ç­›é€‰å®Œæˆ: {len(candidates)} â†’ {len(filtered_candidates)} ä¸ªå€™é€‰è¯")
        
        # ç¬¬äºŒå±‚ï¼šæ¸è¿›å¼ç­›é€‰ - é€ä¸ªä½ç½®åº”ç”¨ç­›é€‰æ¡ä»¶
        print("ğŸ¯ ç¬¬äºŒå±‚ï¼šæ¸è¿›å¼ç­›é€‰...")
        final_candidates = self._progressive_filtering(filtered_candidates, character_filters)
        
        if not final_candidates:
            print("âŒ æ¸è¿›å¼ç­›é€‰åæ— ç»“æœ")
            return []
        
        print(f"   æ¸è¿›å¼ç­›é€‰å®Œæˆ: {len(filtered_candidates)} â†’ {len(final_candidates)} ä¸ªå€™é€‰è¯")
        
        return final_candidates
    
    def _pre_filter_candidates(self, candidates: List[str], character_filters: List[Dict[str, Any]]) -> List[str]:
        """é¢„ç­›é€‰ï¼šä½¿ç”¨æœ€ä¸¥æ ¼çš„æ¡ä»¶å¿«é€Ÿè¿‡æ»¤"""
        
        # ç»Ÿè®¡ç­›é€‰æ¡ä»¶çš„ä¸¥æ ¼ç¨‹åº¦
        strictness_scores = []
        for position, filters in enumerate(character_filters):
            if not filters:
                continue
            
            score = 0
            # å£°è°ƒå’Œå£°æ¯/éŸµæ¯ç»„åˆæœ€ä¸¥æ ¼
            if filters.get('tone') and (filters.get('initial') or filters.get('final')):
                score += 10
            # ç¬”ç”»æ•° + éƒ¨é¦–ç»„åˆä¸¥æ ¼
            elif filters.get('stroke_count') and filters.get('radical'):
                score += 8
            # ç‰¹å®šä½ç½®ç¬”ç”»å¾ˆä¸¥æ ¼
            elif filters.get('stroke_position') and filters.get('contains_stroke'):
                score += 7
            # å•ä¸€æ¡ä»¶
            elif any(filters.values()):
                score += 3
            
            strictness_scores.append((position, score, filters))
        
        # æŒ‰ä¸¥æ ¼ç¨‹åº¦æ’åºï¼Œä¼˜å…ˆä½¿ç”¨æœ€ä¸¥æ ¼çš„æ¡ä»¶
        strictness_scores.sort(key=lambda x: x[1], reverse=True)
        
        if not strictness_scores:
            return candidates
        
        # ä½¿ç”¨æœ€ä¸¥æ ¼çš„æ¡ä»¶è¿›è¡Œé¢„ç­›é€‰
        position, score, filters = strictness_scores[0]
        print(f"   ä½¿ç”¨ç¬¬{position+1}ä¸ªå­—çš„æ¡ä»¶è¿›è¡Œé¢„ç­›é€‰ (ä¸¥æ ¼åº¦: {score})")
        
        # ä¼˜å…ˆä½¿ç”¨å¿«é€Ÿç´¢å¼•æ–¹æ³•è¿›è¡Œé¢„ç­›é€‰ï¼ˆæŒ‰é€‰æ‹©æ€§æ’åºï¼šéŸµæ¯ > å£°æ¯ > éƒ¨é¦– > ç¬”ç”»æ•° > å£°è°ƒï¼‰
        if position == 0 and hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            # ç¬¬ä¸€ä¸ªå­—çš„æ¡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨æœ€å¿«çš„ç´¢å¼•æŸ¥æ‰¾
            # ğŸ”¥ éŸµæ¯æœ€å…·é€‰æ‹©æ€§ï¼Œä¼˜å…ˆä½¿ç”¨
            if filters.get('final'):
                print("     ä½¿ç”¨å¿«é€ŸéŸµæ¯ç´¢å¼•ç­›é€‰...")
                filtered = self.vocab_preprocessor.filter_words_by_final_fast(filters['final'])
            # ğŸ”¥ å£°æ¯æ¬¡ä¹‹
            elif filters.get('initial'):
                print("     ä½¿ç”¨å¿«é€Ÿå£°æ¯ç´¢å¼•ç­›é€‰...")
                filtered = self.vocab_preprocessor.filter_words_by_initial_fast(filters['initial'])
            # éƒ¨é¦–è¾ƒå…·é€‰æ‹©æ€§
            elif filters.get('radical'):
                print("     ä½¿ç”¨å¿«é€Ÿéƒ¨é¦–ç´¢å¼•ç­›é€‰...")
                filtered = self.vocab_preprocessor.filter_words_by_radical_fast(filters['radical'])
            # ç¬”ç”»æ•°é€‰æ‹©æ€§ä¸€èˆ¬
            elif filters.get('stroke_count'):
                print("     ä½¿ç”¨å¿«é€Ÿç¬”ç”»ç´¢å¼•ç­›é€‰...")
                filtered = self.vocab_preprocessor.filter_words_by_stroke_count_fast(filters['stroke_count'])
            # å£°è°ƒé€‰æ‹©æ€§æœ€ä½
            elif filters.get('tone'):
                print("     ä½¿ç”¨å¿«é€Ÿå£°è°ƒç´¢å¼•ç­›é€‰...")
                filtered = self.vocab_preprocessor.filter_words_by_tone_fast(filters['tone'])
            else:
                # å›é€€åˆ°é€šç”¨æ–¹æ³•
                filtered = filter_words_by_advanced_criteria(
                    candidates,
                    character_position=position,
                    initial=filters.get('initial'),
                    final=filters.get('final'), 
                    tone=filters.get('tone'),
                    stroke_count=filters.get('stroke_count'),
                    radical=filters.get('radical'),
                    contains_stroke=filters.get('contains_stroke'),
                    stroke_position=filters.get('stroke_position')
                )
        else:
            # éç¬¬ä¸€ä¸ªå­—ç¬¦æˆ–æ²¡æœ‰é¢„å¤„ç†å™¨ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
            filtered = filter_words_by_advanced_criteria(
                candidates,
                character_position=position,
                initial=filters.get('initial'),
                final=filters.get('final'), 
                tone=filters.get('tone'),
                stroke_count=filters.get('stroke_count'),
                radical=filters.get('radical'),
                contains_stroke=filters.get('contains_stroke'),
                stroke_position=filters.get('stroke_position')
            )
        
        return filtered
    
    def _progressive_filtering(self, candidates: List[str], character_filters: List[Dict[str, Any]]) -> List[str]:
        """æ¸è¿›å¼ç­›é€‰ï¼šé€ä¸ªä½ç½®åº”ç”¨å‰©ä½™çš„ç­›é€‰æ¡ä»¶"""
        
        filtered_words = candidates
        
        for position, filters in enumerate(character_filters):
            if not filters:
                continue
            
            print(f"     ç­›é€‰ç¬¬{position+1}ä¸ªå­—: {filters}")
            
            before_count = len(filtered_words)
            
            # ğŸ”¥ ä¿®å¤ï¼šæ— è®ºå¦‚ä½•éƒ½è¦è¿›è¡Œå®Œæ•´éªŒè¯ï¼Œä¸è·³è¿‡ä»»ä½•æ¡ä»¶
            # é¢„ç­›é€‰åªå¤„ç†ä¸€ä¸ªæœ€ä½³æ¡ä»¶ï¼Œå…¶ä»–æ¡ä»¶å¿…é¡»åœ¨è¿™é‡ŒéªŒè¯
            filtered_words = filter_words_by_advanced_criteria(
                filtered_words,
                character_position=position,
                initial=filters.get('initial'),
                final=filters.get('final'), 
                tone=filters.get('tone'),
                stroke_count=filters.get('stroke_count'),
                radical=filters.get('radical'),
                contains_stroke=filters.get('contains_stroke'),
                stroke_position=filters.get('stroke_position')
            )
            after_count = len(filtered_words)
            print(f"     ç¬¬{position+1}ä¸ªå­—ç­›é€‰: {before_count} â†’ {after_count} ä¸ªå€™é€‰è¯")
            
            # å¦‚æœç­›é€‰åæ²¡æœ‰ç»“æœï¼Œæå‰é€€å‡º
            if not filtered_words:
                print(f"     âŒ ç¬¬{position+1}ä¸ªå­—ç­›é€‰åæ— ç»“æœï¼Œåœæ­¢")
                break
        
        return filtered_words
    
    def _pure_filter_search(self, character_filters: List[Dict[str, Any]], k: int) -> Tuple[List[str], List[float], str]:
        """
        çº¯ç­›é€‰æœç´¢æ¨¡å¼ - ä¸ä¾èµ–embeddingï¼Œåªæ ¹æ®ç­›é€‰æ¡ä»¶ç­›é€‰è¯æ±‡
        """
        print("ğŸ” çº¯ç­›é€‰æ¨¡å¼ï¼šåŸºäºç­›é€‰æ¡ä»¶æœç´¢è¯æ±‡...")
        
        try:
            # è·å–å€™é€‰è¯æ± ï¼Œä¼˜å…ˆçº§ï¼šå¤§è¯åº“ > åŸºç¡€è¯åº“ > ç¦»çº¿ç­›é€‰å™¨
            candidates = None
            source_name = ""
            
            if self.large_vocabulary and len(self.large_vocabulary) > 1000:
                candidates = self.large_vocabulary
                source_name = f"å¤§è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯"
            elif self.candidate_words and len(self.candidate_words) > 0:
                candidates = self.candidate_words
                source_name = f"åŸºç¡€è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯"
            else:
                return [], [], "âŒ æ²¡æœ‰å¯ç”¨çš„è¯åº“æ•°æ®"
            
            print(f"ğŸ“– ä½¿ç”¨{source_name}")
            
            # åº”ç”¨åˆ†å±‚ç­›é€‰
            filtered_words = self._apply_layered_filtering(candidates, character_filters)
            
            if not filtered_words:
                return [], [], "ğŸ¯ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ‰€æœ‰ç­›é€‰æ¡ä»¶çš„è¯æ±‡"
            
            # é™åˆ¶ç»“æœæ•°é‡
            final_results = filtered_words[:k] if len(filtered_words) > k else filtered_words
            
            # ç”Ÿæˆé›¶ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå› ä¸ºæ²¡æœ‰å‚è€ƒè¯æ±‡ï¼‰
            zero_similarities = [0.0] * len(final_results)
            
            # æ„å»ºç»“æœæ¶ˆæ¯
            result_msg = self._build_pure_filter_result_message(final_results, character_filters, len(filtered_words))
            
            print(f"âœ… çº¯ç­›é€‰å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results, zero_similarities, result_msg
            
        except Exception as e:
            error_msg = f"âŒ çº¯ç­›é€‰æœç´¢å¤±è´¥: {str(e)}"
            print(error_msg)
            return [], [], error_msg
    
    def _build_pure_filter_result_message(self, results: List[str], character_filters: List[Dict[str, Any]], total_matches: int) -> str:
        """æ„å»ºçº¯ç­›é€‰ç»“æœæ¶ˆæ¯"""
        
        result_lines = ["ğŸ” çº¯ç­›é€‰æœç´¢ç»“æœ"]
        result_lines.append(f"ğŸ“Š æœç´¢æ¨¡å¼: ä»…åŸºäºç­›é€‰æ¡ä»¶ï¼ˆæ— embeddingè®¡ç®—ï¼‰")
        
        # ç­›é€‰æ¡ä»¶ä¿¡æ¯
        if character_filters:
            condition_info = []
            for i, filters in enumerate(character_filters):
                if not filters:
                    continue
                
                char_conditions = []
                if filters.get('initial'):
                    char_conditions.append(f"å£°æ¯={filters['initial']}")
                if filters.get('final'):
                    char_conditions.append(f"éŸµæ¯={filters['final']}")
                if filters.get('tone'):
                    char_conditions.append(f"å£°è°ƒ={filters['tone']}")
                if filters.get('stroke_count'):
                    char_conditions.append(f"ç¬”ç”»={filters['stroke_count']}")
                if filters.get('radical'):
                    char_conditions.append(f"éƒ¨é¦–={filters['radical']}")
                if filters.get('contains_stroke'):
                    if filters.get('stroke_position'):
                        char_conditions.append(f"ç¬¬{filters['stroke_position']}ç¬”={filters['contains_stroke']}")
                    else:
                        char_conditions.append(f"å«ç¬”ç”»={filters['contains_stroke']}")
                
                # æ–°å¢ï¼šæ”¯æŒå¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶çš„æ˜¾ç¤º
                if filters.get('stroke_positions') and isinstance(filters['stroke_positions'], dict):
                    stroke_pos_conditions = []
                    for pos, stroke in sorted(filters['stroke_positions'].items()):
                        stroke_pos_conditions.append(f"ç¬¬{pos}ç¬”={stroke}")
                    char_conditions.append(', '.join(stroke_pos_conditions))
                
                if char_conditions:
                    condition_info.append(f"ç¬¬{i+1}å­—({', '.join(char_conditions)})")
            
            if condition_info:
                result_lines.append(f"ğŸ¯ ç­›é€‰æ¡ä»¶: {'; '.join(condition_info)}")
        
        result_lines.append(f"ğŸ“ˆ åŒ¹é…ç»Ÿè®¡: å…±æ‰¾åˆ° {total_matches} ä¸ªç¬¦åˆæ¡ä»¶çš„è¯æ±‡")
        result_lines.append(f"ğŸ“ æ˜¾ç¤ºç»“æœ: å‰ {len(results)} ä¸ªè¯æ±‡")
        result_lines.append("")
        
        # ç»“æœåˆ—è¡¨
        for i, word in enumerate(results, 1):
            result_lines.append(f"{i:2d}. {word} (ç­›é€‰åŒ¹é…)")
        
        return "\n".join(result_lines)
    
    def _compute_similarities(self, query_word: str, candidates: List[str], k: int) -> Tuple[List[str], List[float]]:
        """è®¡ç®—æŸ¥è¯¢è¯ä¸å€™é€‰è¯çš„ç›¸ä¼¼åº¦ï¼ˆé«˜æ•ˆç‰ˆï¼‰"""
        
        if not candidates:
            return [], []
        
        try:
            # å¦‚æœå€™é€‰è¯è¾ƒå¤šï¼Œåˆ†æ‰¹å¤„ç†
            max_batch_candidates = 300  # æ¯æ‰¹æœ€å¤§å€™é€‰è¯æ•°é‡
            
            if len(candidates) > max_batch_candidates:
                print(f"ğŸ“Š åˆ†æ‰¹å¤„ç† {len(candidates)} ä¸ªå€™é€‰è¯...")
                
                all_similarities = []
                batch_size = max_batch_candidates
                
                for i in range(0, len(candidates), batch_size):
                    batch_candidates = candidates[i:i + batch_size]
                    print(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(candidates) + batch_size - 1) // batch_size}")
                    
                    batch_similarities = self._compute_batch_similarities(query_word, batch_candidates)
                    all_similarities.extend(batch_similarities)
                
                # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
                all_similarities.sort(key=lambda x: x[1], reverse=True)
                top_k = all_similarities[:k]
                
                synonyms = [pair[0] for pair in top_k]
                sim_scores = [pair[1] for pair in top_k]
                
                return synonyms, sim_scores
            else:
                # å€™é€‰è¯è¾ƒå°‘ï¼Œç›´æ¥å¤„ç†
                print(f"ğŸ§  ç›´æ¥è®¡ç®— {len(candidates)} ä¸ªå€™é€‰è¯çš„ç›¸ä¼¼åº¦")
                return self._compute_batch_similarities_with_topk(query_word, candidates, k)
            
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return [], []
    
    def _compute_batch_similarities(self, query_word: str, candidates: List[str]) -> List[Tuple[str, float]]:
        """è®¡ç®—ä¸€æ‰¹å€™é€‰è¯çš„ç›¸ä¼¼åº¦ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            # é™åˆ¶å•æ¬¡ç¼–ç çš„æœ€å¤§æ•°é‡ï¼ˆQwen APIé™åˆ¶ï¼‰
            max_batch_size = 30  # æŸ¥è¯¢è¯ + å€™é€‰è¯æ€»æ•°ä¸è¶…è¿‡32
            
            if len(candidates) > max_batch_size - 1:
                # å€™é€‰è¯å¤ªå¤šï¼Œåˆ†æ‰¹å¤„ç†
                return self._compute_batch_similarities_fallback(query_word, candidates)
            
            # å‡†å¤‡ç¼–ç è¯æ±‡åˆ—è¡¨ï¼ˆæŸ¥è¯¢è¯ + å€™é€‰è¯ï¼‰
            words_to_encode = [query_word] + candidates
            
            # æ‰¹é‡ç¼–ç 
            embeddings = self.qwen_client.encode(words_to_encode)
            
            if embeddings is None or len(embeddings) != len(words_to_encode):
                raise Exception("ç¼–ç å¤±è´¥æˆ–æ•°é‡ä¸åŒ¹é…")
            
            # åˆ†ç¦»æŸ¥è¯¢è¯å’Œå€™é€‰è¯çš„å‘é‡
            query_embedding = embeddings[0]
            candidate_embeddings = embeddings[1:]
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for i, candidate in enumerate(candidates):
                if i < len(candidate_embeddings):
                    candidate_embedding = candidate_embeddings[i]
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_embedding, candidate_embedding) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding)
                    )
                    similarities.append((candidate, float(similarity)))
            
            return similarities
            
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨åˆ†æ‰¹æ–¹æ³•
            return self._compute_batch_similarities_fallback(query_word, candidates)
    
    def _compute_batch_similarities_fallback(self, query_word: str, candidates: List[str]) -> List[Tuple[str, float]]:
        """å¤‡ç”¨çš„æ‰¹æ¬¡ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•"""
        # å‡†å¤‡ç¼–ç è¯æ±‡åˆ—è¡¨ï¼ˆæŸ¥è¯¢è¯ + å€™é€‰è¯ï¼‰
        words_to_encode = [query_word] + candidates
        
        # åˆ†æ‰¹ç¼–ç ä»¥é¿å…è¶…è¿‡APIé™åˆ¶
        batch_size = 20
        all_embeddings = []
        
        for i in range(0, len(words_to_encode), batch_size):
            batch = words_to_encode[i:i + batch_size]
            batch_embeddings = self.qwen_client.encode(batch)
            
            if batch_embeddings is None:
                print(f"âŒ æ‰¹æ¬¡ç¼–ç å¤±è´¥")
                continue
            
            all_embeddings.extend(batch_embeddings)
        
        if len(all_embeddings) != len(words_to_encode):
            print(f"âš ï¸ ç¼–ç æ•°é‡ä¸åŒ¹é…: {len(all_embeddings)} vs {len(words_to_encode)}")
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        query_embedding = all_embeddings[0]
        candidate_embeddings = all_embeddings[1:]
        
        similarities = []
        for i, candidate in enumerate(candidates):
            if i < len(candidate_embeddings):
                candidate_embedding = candidate_embeddings[i]
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(query_embedding, candidate_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding)
                )
                similarities.append((candidate, float(similarity)))
        
        return similarities
    
    def _compute_batch_similarities_with_topk(self, query_word: str, candidates: List[str], k: int) -> Tuple[List[str], List[float]]:
        """è®¡ç®—å€™é€‰è¯ç›¸ä¼¼åº¦å¹¶ç›´æ¥è¿”å›top-kç»“æœï¼ˆé¿å…é€’å½’è°ƒç”¨ï¼‰"""
        
        # ç›´æ¥ä½¿ç”¨fallbackæ–¹æ³•ï¼Œé¿å…é€’å½’è°ƒç”¨
        similarities = self._compute_batch_similarities_fallback(query_word, candidates)
        
        if not similarities:
            return [], []
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›top-kç»“æœ
        top_k = similarities[:k]
        synonyms = [pair[0] for pair in top_k]
        sim_scores = [pair[1] for pair in top_k]
        
        return synonyms, sim_scores
    
    def _build_unified_result_message(self, query_word: str, synonyms: List[str], similarities: List[float], 
                                    character_finals: Optional[List[str]], character_filters: Optional[List[Dict[str, Any]]], 
                                    is_sorted: bool = True, min_length: Optional[int] = None, max_length: Optional[int] = None) -> str:
        """æ„å»ºç»Ÿä¸€çš„ç»“æœæ¶ˆæ¯ï¼ˆæ”¯æŒç®€å•å’Œé«˜çº§ç­›é€‰ï¼‰"""
        
        if not synonyms:
            return "âŒ æœªæ‰¾åˆ°ç›¸ä¼¼è¯æ±‡"
        
        # åˆ¤æ–­æ˜¯å¦æœ‰æŸ¥è¯¢è¯
        has_query_word = bool(query_word and query_word.strip())
        
        # æ„å»ºç»“æœå±•ç¤º
        result_lines = []
        
        if has_query_word:
            result_lines.append(f"ğŸ” æŸ¥è¯¢è¯æ±‡: {query_word}")
        else:
            result_lines.append(f"ğŸ” çº¯ç­›é€‰æ¨¡å¼ï¼ˆæ— æŸ¥è¯¢è¯ï¼‰")
        
        if is_sorted and has_query_word:
            result_lines.append(f"ğŸ“Š æ•°æ®æ¥æº: Qwen3-Embedding-0.6B (1024ç»´å‘é‡)")
            result_lines.append(f"âš¡ ç®—æ³•: äºŒåˆ†æŸ¥æ‰¾ç­›é€‰ + è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº")
        else:
            result_lines.append(f"ğŸ“Š æ•°æ®æ¥æº: é¢„å¤„ç†è¯åº“ç´¢å¼•")
            result_lines.append(f"âš¡ ç®—æ³•: äºŒåˆ†æŸ¥æ‰¾ç­›é€‰ï¼ˆæœªæ’åºï¼‰")
        
        # ç­›é€‰æ¡ä»¶ä¿¡æ¯ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
        if character_filters:
            # é«˜çº§ç­›é€‰æ¨¡å¼
            filters_info = []
            for i, char_filter in enumerate(character_filters):
                if char_filter:
                    filter_parts = []
                    if char_filter.get('initial'):
                        filter_parts.append(f"å£°æ¯={char_filter['initial']}")
                    if char_filter.get('final'):
                        filter_parts.append(f"éŸµæ¯={char_filter['final']}")
                    if char_filter.get('tone'):
                        filter_parts.append(f"å£°è°ƒ={char_filter['tone']}")
                    if char_filter.get('stroke_count'):
                        filter_parts.append(f"ç¬”ç”»={char_filter['stroke_count']}")
                    if char_filter.get('radical'):
                        filter_parts.append(f"éƒ¨é¦–={char_filter['radical']}")
                    if char_filter.get('contains_stroke'):
                        if char_filter.get('stroke_position'):
                            filter_parts.append(f"ç¬¬{char_filter['stroke_position']}ç¬”={char_filter['contains_stroke']}")
                        else:
                            filter_parts.append(f"å«ç¬”ç”»={char_filter['contains_stroke']}")
                    
                    # æ–°å¢ï¼šæ”¯æŒå¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶çš„æ˜¾ç¤º
                    if char_filter.get('stroke_positions') and isinstance(char_filter['stroke_positions'], dict):
                        stroke_pos_parts = []
                        for pos, stroke in sorted(char_filter['stroke_positions'].items()):
                            stroke_pos_parts.append(f"ç¬¬{pos}ç¬”={stroke}")
                        filter_parts.append(', '.join(stroke_pos_parts))
                    
                    if filter_parts:
                        filters_info.append(f"ç¬¬{i+1}å­—({', '.join(filter_parts)})")
            
            if filters_info:
                result_lines.append(f"ğŸ¯ é«˜çº§ç­›é€‰: {', '.join(filters_info)}")
        
        elif character_finals and any(f for f in character_finals):
            # ç®€å•éŸµæ¯ç­›é€‰æ¨¡å¼
            finals_info = []
            for i, final in enumerate(character_finals):
                if final:
                    finals_info.append(f"ç¬¬{i+1}å­—='{final}'")
            if finals_info:
                result_lines.append(f"ğŸµ éŸµæ¯ç­›é€‰: {', '.join(finals_info)}")
        
        # é•¿åº¦ç­›é€‰ä¿¡æ¯
        if min_length is not None or max_length is not None:
            length_info = []
            if min_length is not None:
                length_info.append(f"æœ€å°é•¿åº¦={min_length}")
            if max_length is not None:
                length_info.append(f"æœ€å¤§é•¿åº¦={max_length}")
            result_lines.append(f"ğŸ“ é•¿åº¦ç­›é€‰: {', '.join(length_info)}")
        
        if is_sorted and has_query_word:
            result_lines.append(f"ğŸ“ æ‰¾åˆ° {len(synonyms)} ä¸ªè¿‘ä¹‰è¯ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰:")
        else:
            result_lines.append(f"ğŸ“ æ‰¾åˆ° {len(synonyms)} ä¸ªåŒ¹é…è¯ï¼ˆç­›é€‰ç»“æœï¼‰:")
        
        result_lines.append("")
        
        # è¯æ±‡åˆ—è¡¨
        for i, (synonym, similarity) in enumerate(zip(synonyms, similarities), 1):
            if is_sorted and has_query_word and similarity > 0:
                percentage = similarity * 100
                result_lines.append(f"{i:2d}. {synonym} ({percentage:.1f}%)")
            else:
                result_lines.append(f"{i:2d}. {synonym}")
        
        return "\n".join(result_lines)
    
    def _build_advanced_result_message(self, query_word: str, synonyms: List[str], similarities: List[float], character_filters: Optional[List[Dict[str, Any]]]) -> str:
        """æ„å»ºé«˜çº§æœç´¢ç»“æœæ¶ˆæ¯"""
        
        if not synonyms:
            return "âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç›¸ä¼¼è¯æ±‡"
        
        # æ„å»ºç»“æœå±•ç¤º
        result_lines = [f"ğŸ” æŸ¥è¯¢è¯æ±‡: {query_word}"]
        result_lines.append(f"ğŸ“Š æ•°æ®æ¥æº: Qwen3-Embedding-0.6B (1024ç»´å‘é‡)")
        result_lines.append(f"ğŸ¯ ç®—æ³•: è¯­ä¹‰ç›¸ä¼¼åº¦ + é«˜çº§æ±‰å­—ç­›é€‰")
        
        # æ£€æŸ¥æ˜¯å¦è¿›è¡Œäº†è¯­ä¹‰æ’åº
        semantic_count = sum(1 for s in similarities if s > 0)
        candidate_count = len(similarities) - semantic_count
        
        if semantic_count > 0 and candidate_count > 0:
            result_lines.append(f"ğŸ“ˆ ç»“æœç±»å‹: {semantic_count}ä¸ªè¯­ä¹‰æ’åºè¯ + {candidate_count}ä¸ªå€™é€‰è¯")
        elif semantic_count > 0:
            result_lines.append(f"ğŸ“ˆ ç»“æœç±»å‹: {semantic_count}ä¸ªè¯­ä¹‰æ’åºçš„åŒä¹‰è¯")
        elif candidate_count > 0:
            result_lines.append(f"ğŸ“ˆ ç»“æœç±»å‹: {candidate_count}ä¸ªå€™é€‰è¯ï¼ˆæœªè¿›è¡Œè¯­ä¹‰æ’åºï¼‰")
        
        # ç­›é€‰æ¡ä»¶ä¿¡æ¯
        if character_filters:
            condition_info = []
            for i, filters in enumerate(character_filters):
                if not filters:
                    continue
                
                char_conditions = []
                if filters.get('initial'):
                    char_conditions.append(f"å£°æ¯={filters['initial']}")
                if filters.get('final'):
                    char_conditions.append(f"éŸµæ¯={filters['final']}")
                if filters.get('tone'):
                    char_conditions.append(f"å£°è°ƒ={filters['tone']}")
                if filters.get('stroke_count'):
                    char_conditions.append(f"ç¬”ç”»={filters['stroke_count']}")
                if filters.get('radical'):
                    char_conditions.append(f"éƒ¨é¦–={filters['radical']}")
                if filters.get('contains_stroke'):
                    if filters.get('stroke_position'):
                        char_conditions.append(f"ç¬¬{filters['stroke_position']}ç¬”={filters['contains_stroke']}")
                    else:
                        char_conditions.append(f"å«ç¬”ç”»={filters['contains_stroke']}")
                
                # æ–°å¢ï¼šæ”¯æŒå¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶çš„æ˜¾ç¤º
                if filters.get('stroke_positions') and isinstance(filters['stroke_positions'], dict):
                    stroke_pos_conditions = []
                    for pos, stroke in sorted(filters['stroke_positions'].items()):
                        stroke_pos_conditions.append(f"ç¬¬{pos}ç¬”={stroke}")
                    char_conditions.append(', '.join(stroke_pos_conditions))
                
                if char_conditions:
                    condition_info.append(f"ç¬¬{i+1}å­—({', '.join(char_conditions)})")
            
            if condition_info:
                result_lines.append(f"ğŸ¯ ç­›é€‰æ¡ä»¶: {'; '.join(condition_info)}")
        
        result_lines.append(f"ğŸ“ æ‰¾åˆ° {len(synonyms)} ä¸ªç¬¦åˆæ¡ä»¶çš„è¿‘ä¹‰è¯:")
        result_lines.append("")
        
        # è¿‘ä¹‰è¯åˆ—è¡¨
        for i, (synonym, similarity) in enumerate(zip(synonyms, similarities), 1):
            if similarity > 0:
                # æœ‰è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
                percentage = similarity * 100
                result_lines.append(f"{i:2d}. {synonym} ({percentage:.1f}%)")
            else:
                # å€™é€‰è¯ï¼ˆæ— è¯­ä¹‰åˆ†æ•°ï¼‰
                result_lines.append(f"{i:2d}. {synonym} (å€™é€‰è¯)")
        
        return "\n".join(result_lines)

class QwenSynonymSearcherV2:
    """å…¼å®¹æ€§åŒ…è£…å™¨ - ä½¿ç”¨V3å®ç°"""
    
    def __init__(self, qwen_client=None):
        self.searcher_v3 = QwenSynonymSearcherV3(qwen_client)
        self.qwen_available = self.searcher_v3.qwen_available
    
    def search_synonyms(self, word: str, k: int = 10, character_finals: Optional[List[str]] = None) -> Tuple[List[str], List[float], str]:
        return self.searcher_v3.search_synonyms(word, k, character_finals)

# å¤„ç†å‡½æ•°
def process_qwen_synonym_search_v3(word: str, k: int, char1_final: str, char2_final: str, char3_final: str, char4_final: str) -> str:
    """å¤„ç†QwenåŒä¹‰è¯æŸ¥è¯¢V3"""
    try:
        searcher = QwenSynonymSearcherV3()
        
        # æ„å»ºéŸµæ¯ç­›é€‰æ¡ä»¶
        character_finals = [char1_final, char2_final, char3_final, char4_final]
        # ç§»é™¤æœ«å°¾çš„ç©ºå­—ç¬¦ä¸²
        while character_finals and not character_finals[-1]:
            character_finals.pop()
        
        synonyms, similarities, result_msg = searcher.search_synonyms(word, k, character_finals)
        return result_msg
        
    except Exception as e:
        return f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"


def process_qwen_synonym_search_advanced(
    word: str, 
    k: int,
    char1_conditions: Dict[str, Any] = None,
    char2_conditions: Dict[str, Any] = None,
    char3_conditions: Dict[str, Any] = None,
    char4_conditions: Dict[str, Any] = None
) -> str:
    """å¤„ç†Qwené«˜çº§åŒä¹‰è¯æŸ¥è¯¢"""
    try:
        searcher = QwenSynonymSearcherV3()
        
        # æ„å»ºç­›é€‰æ¡ä»¶
        character_filters = []
        for conditions in [char1_conditions, char2_conditions, char3_conditions, char4_conditions]:
            if conditions:
                # è¿‡æ»¤æ‰ç©ºå€¼
                filtered_conditions = {k: v for k, v in conditions.items() if v}
                character_filters.append(filtered_conditions)
            else:
                character_filters.append({})
        
        # ç§»é™¤æœ«å°¾çš„ç©ºæ¡ä»¶
        while character_filters and not character_filters[-1]:
            character_filters.pop()
        
        synonyms, similarities, result_msg = searcher.search_synonyms_advanced(word, k, character_filters)
        return result_msg
        
    except Exception as e:
        return f"âŒ é«˜çº§æŸ¥è¯¢å¤±è´¥: {str(e)}"


def get_available_search_options() -> Dict[str, List[str]]:
    """è·å–å¯ç”¨çš„æœç´¢é€‰é¡¹"""
    try:
        return {
            'initials': get_available_initials(),     # å£°æ¯
            'finals': get_available_finals(),         # éŸµæ¯
            'tones': get_available_tones(),           # å£°è°ƒ
            'strokes': get_available_strokes(),       # ç¬”ç”»
            'radicals': get_available_radicals()      # éƒ¨é¦–
        }
    except Exception as e:
        print(f"è·å–æœç´¢é€‰é¡¹å¤±è´¥: {e}")
        return {}


def process_synonym_search_with_stroke_positions(word: str, k: int, character_stroke_positions: List[Dict[int, str]] = None) -> str:
    """
    å¤„ç†å¸¦æœ‰å¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶çš„åŒä¹‰è¯æœç´¢
    
    Args:
        word: æŸ¥è¯¢è¯æ±‡
        k: è¿”å›ç»“æœæ•°é‡
        character_stroke_positions: æ¯ä¸ªå­—ç¬¦ä½ç½®çš„ç¬”ç”»ä½ç½®é™åˆ¶åˆ—è¡¨
            æ ¼å¼: [
                {1: "æ¨ª", 3: "ç«–"},    # ç¬¬ä¸€ä¸ªå­—ï¼šç¬¬1ç”»æ˜¯æ¨ªï¼Œç¬¬3ç”»æ˜¯ç«–
                {2: "æ’‡", 5: "ç‚¹"},    # ç¬¬äºŒä¸ªå­—ï¼šç¬¬2ç”»æ˜¯æ’‡ï¼Œç¬¬5ç”»æ˜¯ç‚¹
            ]
    
    Returns:
        æŸ¥è¯¢ç»“æœå­—ç¬¦ä¸²
    """
    try:
        searcher = QwenSynonymSearcherV3()
        
        if not word or not word.strip():
            return "âŒ è¯·è¾“å…¥æŸ¥è¯¢è¯æ±‡"
        
        if not character_stroke_positions:
            # å¦‚æœæ²¡æœ‰ç¬”ç”»ä½ç½®é™åˆ¶ï¼Œä½¿ç”¨æ™®é€šæœç´¢
            synonyms, similarities, result_msg = searcher.search_synonyms(word, k)
            return result_msg
        
        # æ„å»ºé«˜çº§ç­›é€‰æ¡ä»¶
        character_filters = []
        for stroke_positions in character_stroke_positions:
            if stroke_positions and isinstance(stroke_positions, dict):
                # å°†ç¬”ç”»ä½ç½®å­—å…¸æ·»åŠ åˆ°ç­›é€‰æ¡ä»¶ä¸­
                character_filters.append({'stroke_positions': stroke_positions})
            else:
                # ç©ºæ¡ä»¶
                character_filters.append({})
        
        # ä½¿ç”¨é«˜çº§æœç´¢
        synonyms, similarities, result_msg = searcher.search_synonyms_advanced(word, k, character_filters)
        return result_msg
        
    except Exception as e:
        return f"âŒ ç¬”ç”»ä½ç½®é™åˆ¶æœç´¢å¤±è´¥: {str(e)}"


def process_mixed_advanced_search(word: str, k: int, character_configs: List[Dict[str, Any]] = None) -> str:
    """
    å¤„ç†æ··åˆé«˜çº§æœç´¢ï¼ˆæ”¯æŒæ‰€æœ‰ç±»å‹çš„é™åˆ¶æ¡ä»¶ï¼‰
    
    Args:
        word: æŸ¥è¯¢è¯æ±‡
        k: è¿”å›ç»“æœæ•°é‡
        character_configs: æ¯ä¸ªå­—ç¬¦ä½ç½®çš„å®Œæ•´é…ç½®åˆ—è¡¨
            æ ¼å¼: [
                {  # ç¬¬ä¸€ä¸ªå­—çš„æ‰€æœ‰é™åˆ¶æ¡ä»¶
                    'initial': 'å£°æ¯',
                    'final': 'éŸµæ¯', 
                    'tone': 'å£°è°ƒ',
                    'stroke_count': ç¬”ç”»æ•°,
                    'radical': 'éƒ¨é¦–',
                    'contains_stroke': 'åŒ…å«çš„ç¬”ç”»åç§°',
                    'stroke_position': å•ä¸ªç¬”ç”»ä½ç½®,
                    'stroke_positions': {ä½ç½®1: 'ç¬”ç”»1', ä½ç½®2: 'ç¬”ç”»2', ...}  # å¤šä¸ªç¬”ç”»ä½ç½®
                },
                {  # ç¬¬äºŒä¸ªå­—çš„é™åˆ¶æ¡ä»¶
                    'stroke_positions': {1: "æ¨ª", 2: "ç«–", 7: "æ’‡"}
                }
            ]
    
    Returns:
        æŸ¥è¯¢ç»“æœå­—ç¬¦ä¸²
    """
    try:
        searcher = QwenSynonymSearcherV3()
        
        if not word or not word.strip():
            return "âŒ è¯·è¾“å…¥æŸ¥è¯¢è¯æ±‡"
        
        if not character_configs:
            # å¦‚æœæ²¡æœ‰é™åˆ¶æ¡ä»¶ï¼Œä½¿ç”¨æ™®é€šæœç´¢
            synonyms, similarities, result_msg = searcher.search_synonyms(word, k)
            return result_msg
        
        # ç›´æ¥ä½¿ç”¨æä¾›çš„é…ç½®
        synonyms, similarities, result_msg = searcher.search_synonyms_advanced(word, k, character_configs)
        return result_msg
        
    except Exception as e:
        return f"âŒ æ··åˆé«˜çº§æœç´¢å¤±è´¥: {str(e)}"


if __name__ == "__main__":
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    searcher = QwenSynonymSearcherV3()
    result = searcher.search_synonyms("é«˜å…´", 5, ["ao", ""])
    print(result[2])
    
    print("\n" + "="*60)
    print("æµ‹è¯•å¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶åŠŸèƒ½")
    print("="*60)
    
    # æµ‹è¯•å¤šä¸ªç¬”ç”»ä½ç½®é™åˆ¶
    # ä¾‹ï¼šæœç´¢ä¸"é«˜å…´"ç›¸ä¼¼çš„è¯ï¼Œä½†ç¬¬ä¸€ä¸ªå­—çš„ç¬¬1ç”»æ˜¯æ¨ªï¼Œç¬¬3ç”»æ˜¯æ¨ªï¼Œç¬¬äºŒä¸ªå­—çš„ç¬¬1ç”»æ˜¯ç‚¹
    character_stroke_positions = [
        {1: "æ¨ª", 3: "æ¨ª"},    # ç¬¬ä¸€ä¸ªå­—çš„ç¬”ç”»é™åˆ¶
        {1: "ç‚¹"}              # ç¬¬äºŒä¸ªå­—çš„ç¬”ç”»é™åˆ¶
    ]
    
    result = process_synonym_search_with_stroke_positions("é«˜å…´", 10, character_stroke_positions)
    print(result)
    
    print("\n" + "="*60)
    print("æµ‹è¯•æ··åˆé«˜çº§æœç´¢åŠŸèƒ½")
    print("="*60)
    
    # æµ‹è¯•æ··åˆæ¡ä»¶
    character_configs = [
        {  # ç¬¬ä¸€ä¸ªå­—ï¼šå£°æ¯æ˜¯gï¼ŒåŒæ—¶ç¬¬1ç”»æ˜¯æ¨ªï¼Œç¬¬3ç”»æ˜¯æ¨ª
            'initial': 'g',
            'stroke_positions': {1: "æ¨ª", 3: "æ¨ª"}
        },
        {  # ç¬¬äºŒä¸ªå­—ï¼šéŸµæ¯æ˜¯ingï¼ŒåŒæ—¶ç¬¬1ç”»æ˜¯ç‚¹
            'final': 'ing', 
            'stroke_positions': {1: "ç‚¹"}
        }
    ]
    
    result = process_mixed_advanced_search("é«˜å…´", 10, character_configs)
    print(result)
