#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
QwenåŒä¹‰è¯æœç´¢å™¨ V3ç‰ˆæœ¬ - ä¼˜åŒ–ç‰ˆ
å…ˆæŒ‰éŸµæ¯ç­›é€‰ï¼Œå†è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå¤§å¹…æå‡æŸ¥è¯¢é€Ÿåº¦
"""

import numpy as np
from typing import List, Tuple, Optional, Dict, Any
import re
from qwen_embedding_client import QwenEmbeddingClient
from pinyin_tools import (
    get_word_pinyin, filter_words_by_character_finals, 
    filter_words_by_advanced_criteria, get_available_initials, 
    get_available_finals, get_available_tones, get_available_strokes, 
    get_available_radicals
)
from vocabulary_preprocessor import VocabularyPreprocessor
import json
import os

class QwenSynonymSearcherV3:
    """QwenåŒä¹‰è¯æœç´¢å™¨V3 - å…ˆç­›é€‰å†è®¡ç®—ç‰ˆæœ¬"""
    
    def __init__(self, qwen_client=None):
        """åˆå§‹åŒ–æœç´¢å™¨"""
        print("ğŸš€ åˆå§‹åŒ–QwenSynonymSearcherV3...")
        
        # åˆå§‹åŒ–Qwenå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        if qwen_client:
            self.qwen_client = qwen_client
            self.qwen_available = qwen_client.available
        else:
            try:
                self.qwen_client = QwenEmbeddingClient()
                if self.qwen_client.available:
                    print("âœ… Qwen embeddingæœåŠ¡è¿æ¥æˆåŠŸ")
                    self.qwen_available = True
                else:
                    print("âš ï¸ Qwen embeddingæœåŠ¡ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼")
                    self.qwen_available = False
            except Exception as e:
                print(f"âš ï¸ Qwenå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("ğŸ”§ åˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼ï¼ˆä»…æ”¯æŒç­›é€‰åŠŸèƒ½ï¼‰")
                self.qwen_client = None
                self.qwen_available = False
        
        # åˆå§‹åŒ–è¯åº“é¢„å¤„ç†å™¨
        print("ğŸ”„ åˆå§‹åŒ–è¯åº“é¢„å¤„ç†å™¨...")
        self.vocab_preprocessor = VocabularyPreprocessor()
        
        # åŠ è½½é¢„å¤„ç†çš„è¯åº“æ•°æ®
        if self.vocab_preprocessor.preprocess_vocabulary():
            self.candidate_words = self.vocab_preprocessor.get_all_words()
            print(f"âœ… å·²åŠ è½½é¢„å¤„ç†è¯åº“: {len(self.candidate_words)} ä¸ªè¯æ±‡")
        else:
            # å¤‡ç”¨æ–¹æ³•åŠ è½½å€™é€‰è¯åº“
            print("âš ï¸ é¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½å€™é€‰è¯...")
            self.candidate_words = self._load_candidate_words()
            if self.candidate_words:
                print(f"ğŸ“š å·²åŠ è½½å€™é€‰è¯åº“: {len(self.candidate_words)} ä¸ªè¯æ±‡")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å€™é€‰è¯åº“æ•°æ®")
        
        # å°è¯•åŠ è½½å¤§è¯åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.large_vocabulary = self._load_large_vocabulary()
        if self.large_vocabulary:
            print(f"ğŸ“– å·²åŠ è½½å¤§è¯åº“: {len(self.large_vocabulary)} ä¸ªè¯æ±‡")
        
        # åˆå§‹åŒ–ç¦»çº¿ç­›é€‰å™¨ï¼ˆå¤‡ç”¨ï¼‰
        try:
            from offline_character_filter import OfflineCharacterFilter
            self.offline_filter = OfflineCharacterFilter()
            print("âœ… ç¦»çº¿ç­›é€‰å™¨å·²å°±ç»ª")
        except Exception as e:
            print(f"âš ï¸ ç¦»çº¿ç­›é€‰å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.offline_filter = None
    
    def get_word_pinyin_fast(self, word: str) -> List[str]:
        """
        å¿«é€Ÿè·å–è¯æ±‡æ‹¼éŸ³ï¼ˆä½¿ç”¨é¢„å¤„ç†çš„æ•°æ®ï¼‰
        
        Args:
            word: è¯æ±‡
            
        Returns:
            æ‹¼éŸ³åˆ—è¡¨
        """
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            pinyins = self.vocab_preprocessor.get_word_pinyin(word)
            if pinyins:
                return pinyins
        
        # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨åŸå§‹å‡½æ•°
        return get_word_pinyin(word)
    
    def _load_candidate_words(self) -> List[str]:
        """åŠ è½½åŸºç¡€å€™é€‰è¯åº“ - ä»…ä»å®é™…æ•°æ®æºåŠ è½½ï¼Œä¸ç¼–é€ """
        try:
            # å°è¯•ä»ci.jsonåŠ è½½è¯æ±‡
            import os
            import json
            
            ci_path = os.path.join(os.path.dirname(__file__), "ci.json")
            if os.path.exists(ci_path):
                with open(ci_path, 'r', encoding='utf-8') as f:
                    ci_data = json.load(f)
                    words = [item.get('ci', '').strip() for item in ci_data if item.get('ci') and len(item.get('ci', '').strip()) >= 2]
                    return words[:500]  # é™åˆ¶ä¸ºå‰500ä¸ªä½œä¸ºåŸºç¡€è¯åº“
            
            print("âš ï¸ æœªæ‰¾åˆ°ci.jsonè¯åº“æ–‡ä»¶")
            return []
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å€™é€‰è¯åº“å¤±è´¥: {e}")
            return []
    
    def _load_large_vocabulary(self) -> Optional[List[str]]:
        """å°è¯•åŠ è½½å¤§è¯åº“ï¼ˆci.jsonï¼‰"""
        try:
            ci_path = "ci.json"
            if os.path.exists(ci_path):
                with open(ci_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                words = []
                seen = set()
                for item in data:
                    word = item.get('ci', '').strip()
                    if word and len(word) >= 2 and word not in seen:
                        words.append(word)
                        seen.add(word)
                return words
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å¤§è¯åº“å¤±è´¥: {e}")
        return None
    
    def _is_chinese(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
        chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
        return bool(chinese_pattern.search(text))
    
    def search_synonyms(self, word: str, k: int = 10, character_finals: Optional[List[str]] = None, 
                       character_filters: Optional[List[Dict[str, Any]]] = None, 
                       min_length: Optional[int] = None, max_length: Optional[int] = None) -> Tuple[List[str], List[float], str]:
        """
        åŒä¹‰è¯æŸ¥è¯¢ - å®Œå…¨ç»Ÿä¸€å¤„ç†ç‰ˆæœ¬
        æ­¥éª¤ï¼š1.äºŒåˆ†æŸ¥æ‰¾è·å–å€™é€‰è¯ â†’ 2.éªŒè¯æ‰€æœ‰æ¡ä»¶ â†’ 3.æ ¹æ®æ˜¯å¦æœ‰æŸ¥è¯¢è¯å†³å®šæ˜¯å¦ç›¸ä¼¼åº¦æ’åº
        
        Args:
            word: æŸ¥è¯¢è¯æ±‡ï¼ˆå¯ä»¥ä¸ºç©ºï¼Œè¡¨ç¤ºçº¯ç­›é€‰ï¼‰
            k: è¿”å›ç»“æœæ•°é‡
            character_finals: æ¯ä¸ªå­—ç¬¦ä½ç½®çš„éŸµæ¯è¦æ±‚ï¼Œç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ— è¦æ±‚ï¼ˆç®€å•æ¨¡å¼ï¼‰
            character_filters: é«˜çº§ç­›é€‰æ¡ä»¶ï¼Œæ¯ä¸ªä½ç½®çš„è¯¦ç»†ç­›é€‰å‚æ•°ï¼ˆé«˜çº§æ¨¡å¼ï¼‰
            min_length: æœ€å°è¯é•¿é™åˆ¶ï¼ˆå¯é€‰ï¼‰
            max_length: æœ€å¤§è¯é•¿é™åˆ¶ï¼ˆå¯é€‰ï¼‰
        """
        try:
            # åˆ¤æ–­æ˜¯å¦æœ‰æŸ¥è¯¢è¯
            has_query_word = bool(word and word.strip())
            
            if has_query_word:
                word = word.strip()
                if not self._is_chinese(word):
                    return [], [], "âš ï¸ è¾“å…¥çš„è¯æ±‡ä¸æ˜¯ä¸­æ–‡"
                print(f"ğŸ” å¼€å§‹æŸ¥è¯¢: {word}")
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç­›é€‰æ¡ä»¶ï¼ˆéŸµæ¯ã€é«˜çº§ç­›é€‰ã€é•¿åº¦ç­›é€‰ï¼‰
                has_any_filter = False
                if character_finals and any(f for f in character_finals):
                    has_any_filter = True
                if character_filters and any(f for f in character_filters):
                    has_any_filter = True
                if min_length is not None or max_length is not None:
                    has_any_filter = True
                    
                if not has_any_filter:
                    return [], [], "âŒ è¯·è¾“å…¥æŸ¥è¯¢è¯æ±‡æˆ–è®¾ç½®ç­›é€‰æ¡ä»¶"
                print("ğŸ” å¯åŠ¨çº¯ç­›é€‰æ¨¡å¼ï¼ˆæ— æŸ¥è¯¢è¯ï¼‰")
            
            # æ­¥éª¤1: ç»Ÿä¸€ç­›é€‰å€™é€‰è¯
            print("âš¡ æ­¥éª¤1: é«˜æ•ˆç­›é€‰å€™é€‰è¯...")
            
            # ğŸ”„ ç»Ÿä¸€ç­›é€‰é€»è¾‘ï¼šæ”¯æŒç®€å•éŸµæ¯ç­›é€‰ã€é«˜çº§ç­›é€‰å’Œçº¯é•¿åº¦ç­›é€‰
            if character_filters:
                # é«˜çº§ç­›é€‰æ¨¡å¼
                eligible_words = self._filter_candidates_by_advanced_criteria(word, character_filters)
            elif character_finals and any(f for f in character_finals):
                # ç®€å•éŸµæ¯ç­›é€‰æ¨¡å¼
                eligible_words = self._filter_candidates_by_finals(word, character_finals)
            else:
                # çº¯é•¿åº¦ç­›é€‰æ¨¡å¼ï¼ˆæ— å…¶ä»–ç­›é€‰æ¡ä»¶ï¼‰
                eligible_words = self._filter_candidates_by_length_only(word)
            
            # ğŸ“ åº”ç”¨é•¿åº¦ç­›é€‰
            if min_length is not None or max_length is not None:
                eligible_words = self._filter_by_length(eligible_words, min_length, max_length)
                print(f"ğŸ“ é•¿åº¦ç­›é€‰å: {len(eligible_words)} ä¸ªå€™é€‰è¯")
            
            if not eligible_words:
                if character_finals and any(f for f in character_finals):
                    return [], [], "ğŸµ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆéŸµæ¯æ¡ä»¶çš„è¯æ±‡"
                elif character_filters:
                    return [], [], "ğŸ¯ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆç­›é€‰æ¡ä»¶çš„è¯æ±‡"
                else:
                    return [], [], "âŒ æ²¡æœ‰æ‰¾åˆ°å€™é€‰è¯æ±‡"
            
            print(f"âœ… ç­›é€‰å‡º {len(eligible_words)} ä¸ªå€™é€‰è¯")
            
            # æ­¥éª¤2: ç»Ÿä¸€æ’åºå†³ç­–
            if has_query_word and self.qwen_available:
                # æœ‰æŸ¥è¯¢è¯ä¸”æœåŠ¡å¯ç”¨ï¼šè®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
                print("ğŸ§  æ­¥éª¤2: è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº...")
                synonyms, similarities = self._compute_similarities_and_sort(word, eligible_words, k)
                
                if not synonyms:
                    return [], [], "âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥"
                
                result_msg = self._build_unified_result_message(word, synonyms, similarities, character_finals, character_filters, sorted=True, min_length=min_length, max_length=max_length)
                return synonyms, similarities, result_msg
            
            else:
                # æ— æŸ¥è¯¢è¯æˆ–æœåŠ¡ä¸å¯ç”¨ï¼šç›´æ¥è¿”å›ç­›é€‰ç»“æœ
                if has_query_word:
                    print("âš ï¸ QwenæœåŠ¡ä¸å¯ç”¨ï¼Œè¿”å›ç­›é€‰ç»“æœï¼ˆæ— æ’åºï¼‰")
                else:
                    print("ğŸ“‹ è¿”å›ç­›é€‰ç»“æœï¼ˆçº¯ç­›é€‰æ¨¡å¼ï¼‰")
                
                final_results = eligible_words[:k]
                zero_similarities = [0.0] * len(final_results)
                result_msg = self._build_unified_result_message(word, final_results, zero_similarities, character_finals, character_filters, sorted=False, min_length=min_length, max_length=max_length)
                return final_results, zero_similarities, result_msg
            
        except Exception as e:
            error_msg = f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"
            print(error_msg)
            return [], [], error_msg
    
    def _compute_similarities_and_sort(self, query_word: str, candidates: List[str], k: int) -> Tuple[List[str], List[float]]:
        """
        è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åºï¼Œè¿”å›å‰kä¸ªç»“æœ
        ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥è®¡ç®—å¹¶æ’åºï¼Œé¿å…ä¸­é—´æ­¥éª¤
        """
        
        if not candidates:
            return [], []
        
        try:
            # é™åˆ¶å€™é€‰è¯æ•°é‡ä»¥æé«˜æ•ˆç‡
            max_candidates = 1000
            if len(candidates) > max_candidates:
                print(f"ğŸ“Š å€™é€‰è¯è¿‡å¤š ({len(candidates)}ä¸ª)ï¼Œé™åˆ¶ä¸ºå‰{max_candidates}ä¸ª")
                candidates = candidates[:max_candidates]
            
            print(f"ğŸ§  è®¡ç®— {len(candidates)} ä¸ªå€™é€‰è¯çš„ç›¸ä¼¼åº¦...")
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = self._compute_batch_similarities_optimized(query_word, candidates)
            
            if not similarities:
                print("âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥")
                return [], []
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # è¿”å›å‰kä¸ªç»“æœ
            top_k = similarities[:k]
            synonyms = [pair[0] for pair in top_k]
            sim_scores = [pair[1] for pair in top_k]
            
            print(f"âœ… æ’åºå®Œæˆï¼Œè¿”å›å‰{len(synonyms)}ä¸ªç»“æœ")
            return synonyms, sim_scores
            
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return [], []
    
    def _compute_batch_similarities_optimized(self, query_word: str, candidates: List[str]) -> List[Tuple[str, float]]:
        """ä¼˜åŒ–çš„æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—"""
        try:
            # å‡†å¤‡ç¼–ç è¯æ±‡åˆ—è¡¨ï¼ˆæŸ¥è¯¢è¯ + å€™é€‰è¯ï¼‰
            words_to_encode = [query_word] + candidates
            
            # åˆ†æ‰¹ç¼–ç ä»¥é¿å…è¶…è¿‡APIé™åˆ¶
            batch_size = 25  # ä¿å®ˆçš„æ‰¹æ¬¡å¤§å°
            all_embeddings = []
            
            total_batches = (len(words_to_encode) + batch_size - 1) // batch_size
            
            for i in range(0, len(words_to_encode), batch_size):
                batch = words_to_encode[i:i + batch_size]
                print(f"   ç¼–ç æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches}: {len(batch)} ä¸ªè¯")
                
                batch_embeddings = self.qwen_client.encode(batch)
                
                if batch_embeddings is None:
                    print(f"âŒ æ‰¹æ¬¡ç¼–ç å¤±è´¥")
                    continue
                
                all_embeddings.extend(batch_embeddings)
            
            if len(all_embeddings) != len(words_to_encode):
                print(f"âš ï¸ ç¼–ç æ•°é‡ä¸åŒ¹é…: {len(all_embeddings)} vs {len(words_to_encode)}")
                return []
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            query_embedding = all_embeddings[0]
            candidate_embeddings = all_embeddings[1:]
            
            similarities = []
            for i, candidate in enumerate(candidates):
                if i < len(candidate_embeddings):
                    candidate_embedding = candidate_embeddings[i]
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_embedding, candidate_embedding) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding)
                    )
                    similarities.append((candidate, float(similarity)))
            
            return similarities
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return []
    
    def search_synonyms_advanced(
        self, 
        word: str, 
        k: int = 10,
        character_filters: Optional[List[Dict[str, Any]]] = None
    ) -> Tuple[List[str], List[float], str]:
        """
        é«˜çº§åŒä¹‰è¯æŸ¥è¯¢ - æ”¯æŒå£°æ¯ã€éŸµæ¯ã€å£°è°ƒã€ç¬”ç”»ã€éƒ¨é¦–ç­‰å¤šé‡ç­›é€‰
        
        Args:
            word: æŸ¥è¯¢è¯æ±‡ï¼ˆå¯ä»¥ä¸ºç©ºï¼Œè¡¨ç¤ºçº¯ç­›é€‰æ¨¡å¼ï¼‰
            k: è¿”å›ç»“æœæ•°é‡
            character_filters: æ¯ä¸ªå­—ç¬¦ä½ç½®çš„ç­›é€‰æ¡ä»¶åˆ—è¡¨
                æ ¼å¼: [
                    {  # ç¬¬ä¸€ä¸ªå­—çš„æ¡ä»¶
                        'initial': 'å£°æ¯',
                        'final': 'éŸµæ¯', 
                        'tone': 'å£°è°ƒ',
                        'stroke_count': ç¬”ç”»æ•°,
                        'radical': 'éƒ¨é¦–',
                        'contains_stroke': 'ç¬”ç”»åç§°',
                        'stroke_position': ç¬”ç”»ä½ç½®
                    },
                    {  # ç¬¬äºŒä¸ªå­—çš„æ¡ä»¶
                        ...
                    }
                ]
        
        Returns:
            (åŒä¹‰è¯åˆ—è¡¨, ç›¸ä¼¼åº¦åˆ—è¡¨, ç»“æœæ¶ˆæ¯)
        """
        try:
            # ğŸš€ æ–°å¢ï¼šæ”¯æŒçº¯ç­›é€‰æ¨¡å¼ï¼ˆæŸ¥è¯¢è¯ä¸ºç©ºï¼‰
            if not word or not word.strip():
                if character_filters:
                    print("ğŸ” å¯åŠ¨çº¯ç­›é€‰æ¨¡å¼ï¼ˆæ— éœ€embeddingæœåŠ¡ï¼‰")
                    return self._pure_filter_search(character_filters, k)
                else:
                    return [], [], "âŒ è¯·è¾“å…¥æŸ¥è¯¢è¯æ±‡æˆ–è®¾ç½®ç­›é€‰æ¡ä»¶"
            
            word = word.strip()
            if not self._is_chinese(word):
                return [], [], "âš ï¸ è¾“å…¥çš„è¯æ±‡ä¸æ˜¯ä¸­æ–‡"
            
            print(f"ğŸ” å¼€å§‹é«˜çº§æŸ¥è¯¢: {word}")
            
            # æ­¥éª¤1: æŒ‰é«˜çº§æ¡ä»¶ç­›é€‰å€™é€‰è¯
            print("ğŸ“ æ­¥éª¤1: æŒ‰é«˜çº§æ¡ä»¶ç­›é€‰å€™é€‰è¯...")
            eligible_words = self._filter_candidates_by_advanced_criteria(word, character_filters)
            
            if not eligible_words:
                # ğŸš€ æ–°æœºåˆ¶ï¼šæ²¡æœ‰ç¬¦åˆç­›é€‰æ¡ä»¶çš„è¯æ—¶ï¼Œç›´æ¥è¿”å›å‰kä¸ªå€™é€‰è¯ï¼ˆä¸è¿›è¡Œè¯­ä¹‰æ’åºï¼‰
                print("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆç­›é€‰æ¡ä»¶çš„è¯æ±‡ï¼Œè¿”å›å‰kä¸ªå€™é€‰è¯")
                candidates = self._get_candidate_pool(word)
                # ç§»é™¤æŸ¥è¯¢è¯æœ¬èº«ï¼Œå–å‰kä¸ª
                top_candidates = [w for w in candidates if w != word][:k]
                
                # è¿”å›ç©ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè¡¨ç¤ºæœªè¿›è¡Œè¯­ä¹‰è®¡ç®—ï¼‰
                zero_similarities = [0.0] * len(top_candidates)
                result_msg = f"ğŸ¯ æœªæ‰¾åˆ°ç¬¦åˆç­›é€‰æ¡ä»¶çš„è¯æ±‡ï¼Œæ˜¾ç¤ºå‰{len(top_candidates)}ä¸ªå€™é€‰è¯ï¼ˆæœªæ’åºï¼‰"
                
                return top_candidates, zero_similarities, result_msg
            
            print(f"âœ… ç­›é€‰å‡º {len(eligible_words)} ä¸ªå€™é€‰è¯")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
            if not self.qwen_available:
                print("âš ï¸ QwenæœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡è¯­ä¹‰æ’åº")
                # ç›´æ¥è¿”å›ç­›é€‰ç»“æœï¼Œä¸è¿›è¡Œè¯­ä¹‰æ’åº
                final_results = eligible_words[:k]
                zero_similarities = [0.0] * len(final_results)
                result_msg = self._build_advanced_result_message(word, final_results, zero_similarities, character_filters)
                return final_results, zero_similarities, result_msg
            
            # æ­¥éª¤2: å¯¹ç­›é€‰åçš„è¯æ±‡è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
            print("ğŸ§  æ­¥éª¤2: è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº...")
            synonyms, similarities = self._compute_similarities(word, eligible_words, k)
            
            if not synonyms:
                return [], [], "âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥"
            
            # æ„å»ºç»“æœæ¶ˆæ¯
            result_msg = self._build_advanced_result_message(word, synonyms, similarities, character_filters)
            
            return synonyms, similarities, result_msg
            
        except Exception as e:
            error_msg = f"âŒ é«˜çº§æŸ¥è¯¢å¤±è´¥: {str(e)}"
            print(error_msg)
            return [], [], error_msg
    
    def _get_candidate_pool(self, query_word: str) -> List[str]:
        """è·å–å€™é€‰è¯æ± ï¼ˆç”¨äºç›¸ä¼¼åº¦è®¡ç®—ï¼‰"""
        
        # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†çš„è¯æ±‡åº“
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            candidates = self.vocab_preprocessor.get_all_words()
            print(f"âš¡ ä½¿ç”¨é¢„å¤„ç†è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯")
        elif self.large_vocabulary and len(self.large_vocabulary) > 1000:
            # ä½¿ç”¨å¤§è¯åº“
            candidates = self.large_vocabulary
            print(f"ğŸ“– ä½¿ç”¨å¤§è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯")
        else:
            candidates = self.candidate_words
            print(f"ğŸ“š ä½¿ç”¨åŸºç¡€è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯")
        
        # è¿‡æ»¤æ‰æŸ¥è¯¢è¯æœ¬èº«å’Œå•å­—è¯
        filtered = [w for w in candidates if w != query_word and len(w) >= 2]
        return filtered
    
    def _filter_candidates_by_finals(self, query_word: str, character_finals: Optional[List[str]]) -> List[str]:
        """æŒ‰éŸµæ¯æ¡ä»¶ç­›é€‰å€™é€‰è¯ï¼ˆé«˜æ•ˆç‰ˆ - ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ä¼˜åŒ–ï¼‰"""
        
        # åˆ¤æ–­æ˜¯å¦æœ‰æŸ¥è¯¢è¯
        has_query_word = bool(query_word and query_word.strip())
        
        # å¦‚æœæ²¡æœ‰éŸµæ¯é™åˆ¶ï¼Œè¿”å›åŸºç¡€è¯åº“ï¼ˆé«˜è´¨é‡è¯æ±‡ï¼‰
        if not character_finals or all(not f for f in character_finals):
            if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
                base_candidates = self.vocab_preprocessor.filter_words_by_length(2, 10)
                if has_query_word:
                    base_candidates = [w for w in base_candidates if w != query_word]
                print(f"âš¡ æ— éŸµæ¯é™åˆ¶ï¼Œä½¿ç”¨é¢„å¤„ç†è¯åº“: {len(base_candidates)} ä¸ªå€™é€‰è¯")
                return base_candidates[:1000]  # é™åˆ¶æ•°é‡
            else:
                # å¤‡ç”¨æ–¹æ³•
                candidates = self._get_candidate_pool(query_word if has_query_word else "")
                base_candidates = [w for w in self.candidate_words if len(w) >= 2]
                if has_query_word:
                    base_candidates = [w for w in base_candidates if w != query_word]
                if len(base_candidates) < 500 and len(candidates) > len(base_candidates):
                    additional = [w for w in candidates[:1000] if w not in base_candidates]
                    base_candidates.extend(additional)
                print(f"ğŸ“š æ— éŸµæ¯é™åˆ¶ï¼Œä½¿ç”¨å¤‡ç”¨è¯åº“: {len(base_candidates)} ä¸ªå€™é€‰è¯")
                return base_candidates
        
        print("âš¡ åº”ç”¨é«˜æ•ˆéŸµæ¯ç­›é€‰...")
        
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            return self._efficient_filter_by_finals(query_word, character_finals)
        else:
            # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨åŸå§‹ç­›é€‰
            candidates = self._get_candidate_pool(query_word if has_query_word else "")
            filtered_words = filter_words_by_character_finals(candidates, character_finals)
            print(f"ğŸ“š å¤‡ç”¨ç­›é€‰å®Œæˆ: {len(filtered_words)} ä¸ªå€™é€‰è¯")
            return filtered_words
    
    def _filter_candidates_by_length_only(self, query_word: str) -> List[str]:
        """çº¯é•¿åº¦ç­›é€‰ï¼ˆæ— å…¶ä»–æ¡ä»¶æ—¶ä½¿ç”¨ï¼‰"""
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            # è·å–åŸºç¡€è¯åº“ï¼ˆ2-10å­—ï¼‰
            base_candidates = self.vocab_preprocessor.filter_words_by_length(2, 10)
            # ç§»é™¤æŸ¥è¯¢è¯æœ¬èº«
            if query_word and query_word.strip():
                base_candidates = [w for w in base_candidates if w != query_word]
            print(f"âš¡ çº¯é•¿åº¦ç­›é€‰ï¼Œä½¿ç”¨é¢„å¤„ç†è¯åº“: {len(base_candidates)} ä¸ªå€™é€‰è¯")
            return base_candidates[:1000]  # é™åˆ¶æ•°é‡é¿å…è®¡ç®—é‡è¿‡å¤§
        else:
            # å¤‡ç”¨æ–¹æ³•
            candidates = [w for w in self.candidate_words if len(w) >= 2]
            if query_word and query_word.strip():
                candidates = [w for w in candidates if w != query_word]
            print(f"ğŸ“š çº¯é•¿åº¦ç­›é€‰ï¼Œä½¿ç”¨å¤‡ç”¨è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯")
            return candidates[:1000]
    
    def _efficient_filter_by_finals(self, query_word: str, character_finals: List[str]) -> List[str]:
        """
        é«˜æ•ˆçš„éŸµæ¯ç­›é€‰ç®—æ³•ï¼š
        1. äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯
        2. é€è¯éªŒè¯æ‰€æœ‰æ¡ä»¶
        """
        print(f"   ç›®æ ‡éŸµæ¯æ¡ä»¶: {character_finals}")
        
        # æ­¥éª¤1: ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯é›†åˆ
        initial_candidates = self._get_candidates_by_binary_search(character_finals)
        
        if not initial_candidates:
            print("   äºŒåˆ†æŸ¥æ‰¾æ— ç»“æœ")
            return []
        
        print(f"   äºŒåˆ†æŸ¥æ‰¾ç»“æœ: {len(initial_candidates)} ä¸ªå€™é€‰è¯")
        
        # æ­¥éª¤2: é€è¯éªŒè¯æ‰€æœ‰æ¡ä»¶
        verified_candidates = self._verify_candidates_conditions(initial_candidates, character_finals)
        
        # ç§»é™¤æŸ¥è¯¢è¯æœ¬èº«ï¼ˆå¦‚æœæœ‰æŸ¥è¯¢è¯ï¼‰
        if query_word and query_word.strip():
            verified_candidates = [w for w in verified_candidates if w != query_word]
        
        print(f"   éªŒè¯å®Œæˆ: {len(verified_candidates)} ä¸ªç¬¦åˆæ¡ä»¶çš„è¯")
        return verified_candidates
    
    def _get_candidates_by_binary_search(self, character_finals: List[str]) -> List[str]:
        """ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯é›†åˆ"""
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªéç©ºçš„éŸµæ¯æ¡ä»¶ç”¨äºäºŒåˆ†æŸ¥æ‰¾
        primary_final = None
        primary_position = -1
        
        for i, final in enumerate(character_finals):
            if final and final.strip():
                primary_final = final.strip()
                primary_position = i
                break
        
        if not primary_final:
            # æ²¡æœ‰éŸµæ¯æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰è¯æ±‡
            return self.vocab_preprocessor.get_all_words()
        
        print(f"   ä½¿ç”¨ç¬¬{primary_position+1}ä¸ªå­—çš„éŸµæ¯'{primary_final}'è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾")
        
        # ä½¿ç”¨é¢„å¤„ç†å™¨çš„éŸµæ¯ç´¢å¼•å¿«é€Ÿè·å–å€™é€‰è¯
        candidates = self.vocab_preprocessor.filter_words_by_final_fast(primary_final)
        
        # å¦‚æœå€™é€‰è¯å¤ªå°‘ï¼Œå°è¯•ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ‰©å±•
        if len(candidates) < 100:
            # å°è¯•æŒ‰æ‹¼éŸ³å‰ç¼€æœç´¢ï¼ˆç”¨äºå¤„ç†å¤åˆéŸµæ¯ï¼‰
            prefix_candidates = self.vocab_preprocessor.binary_search_by_pinyin_prefix(primary_final)
            # åˆå¹¶ç»“æœå¹¶å»é‡
            all_candidates = list(set(candidates + prefix_candidates))
            print(f"   æ‰©å±•æœç´¢: {len(candidates)} + {len(prefix_candidates)} = {len(all_candidates)} ä¸ªå€™é€‰è¯")
            return all_candidates
        
        return candidates
    
    def _verify_candidates_conditions(self, candidates: List[str], character_finals: List[str]) -> List[str]:
        """é€è¯éªŒè¯å€™é€‰è¯æ˜¯å¦æ»¡è¶³æ‰€æœ‰éŸµæ¯æ¡ä»¶"""
        
        verified = []
        target_length = len(character_finals)
        
        # å¯¼å…¥ pinyin_tools ä»¥è·å–å®æ—¶æ‹¼éŸ³
        from pinyin_tools import get_word_finals
        
        # é™åˆ¶å€™é€‰è¯æ•°é‡ä»¥æé«˜æ€§èƒ½
        max_candidates = 1000
        if len(candidates) > max_candidates:
            candidates = candidates[:max_candidates]
            print(f"   ä¸ºæé«˜æ€§èƒ½ï¼Œé™åˆ¶éªŒè¯å€™é€‰è¯ä¸ºå‰{max_candidates}ä¸ª")
        
        for word in candidates:
            # æ£€æŸ¥è¯é•¿æ˜¯å¦åŒ¹é…
            if len(word) != target_length:
                continue
            
            # ä½¿ç”¨ pinyin_tools è·å–å®æ—¶éŸµæ¯
            try:
                actual_finals = get_word_finals(word)
                
                # è¿‡æ»¤æ‰éæ‹¼éŸ³æ•°æ®ï¼ˆå¦‚"èª¬"ç­‰æ³¨é‡Šï¼‰
                clean_finals = []
                for final in actual_finals:
                    # åªä¿ç•™åŒ…å«æ‹¼éŸ³å­—ç¬¦çš„éŸµæ¯
                    if final and all(c.isalpha() or c in 'Ã¼É¡' for c in final):
                        clean_finals.append(final)
                
                # æ£€æŸ¥æ¸…ç†åçš„éŸµæ¯æ•°é‡æ˜¯å¦åŒ¹é…
                if len(clean_finals) != target_length:
                    continue
                
                # éªŒè¯æ¯ä¸ªä½ç½®çš„éŸµæ¯
                matches = True
                for i, (required_final, actual_final) in enumerate(zip(character_finals, clean_finals)):
                    if required_final and required_final.strip():
                        required = required_final.strip()
                        
                        # æ ‡å‡†åŒ–éŸµæ¯æ¯”è¾ƒï¼šå¤„ç†Unicodeå­—ç¬¦É¡å’Œueâ†”veè½¬æ¢
                        def normalize_final(final):
                            """æ ‡å‡†åŒ–éŸµæ¯æ ¼å¼ï¼Œæ”¯æŒueâ†”veåŒå‘è½¬æ¢"""
                            if not final:
                                return ''
                            # å¤„ç†Unicode É¡å­—ç¬¦
                            normalized = final.replace('É¡', 'g')
                            # å¤„ç†ueâ†”veè½¬æ¢ï¼šéƒ½ç»Ÿä¸€ä¸ºueè¿›è¡Œæ¯”è¾ƒ
                            if normalized == 've':
                                normalized = 'ue'
                            return normalized
                        
                        if normalize_final(actual_final) != normalize_final(required):
                            matches = False
                            break
                
                if matches:
                    verified.append(word)
                    # æ‰¾åˆ°è¶³å¤Ÿçš„è¯æ±‡å°±åœæ­¢ï¼ˆä¸ºäº†æ€§èƒ½ï¼‰
                    if len(verified) >= 100:
                        print(f"   å·²æ‰¾åˆ°{len(verified)}ä¸ªç¬¦åˆæ¡ä»¶çš„è¯ï¼Œåœæ­¢éªŒè¯ä»¥æé«˜æ€§èƒ½")
                        break
                        
            except Exception:
                # å¦‚æœæ‹¼éŸ³è·å–å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªè¯
                continue
        
        return verified
    
    def _filter_candidates_by_advanced_criteria(self, query_word: str, character_filters: Optional[List[Dict[str, Any]]]) -> List[str]:
        """æŒ‰é«˜çº§æ¡ä»¶ç­›é€‰å€™é€‰è¯ï¼ˆé«˜æ•ˆç‰ˆ - ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ä¼˜åŒ–ï¼‰"""
        
        # å¦‚æœæ²¡æœ‰ç­›é€‰æ¡ä»¶ï¼Œè¿”å›åŸºç¡€è¯åº“
        if not character_filters:
            if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
                base_candidates = self.vocab_preprocessor.filter_words_by_length(2, 10)
                base_candidates = [w for w in base_candidates if w != query_word]
                print(f"âš¡ æ— ç­›é€‰æ¡ä»¶ï¼Œä½¿ç”¨é¢„å¤„ç†è¯åº“: {len(base_candidates)} ä¸ªå€™é€‰è¯")
                return base_candidates[:1000]
            else:
                candidates = self._get_candidate_pool(query_word)
                base_candidates = [w for w in self.candidate_words if w != query_word and len(w) >= 2]
                if len(base_candidates) < 500 and len(candidates) > len(base_candidates):
                    additional = [w for w in candidates[:1000] if w not in base_candidates]
                    base_candidates.extend(additional)
                print(f"ğŸ“š æ— ç­›é€‰æ¡ä»¶ï¼Œä½¿ç”¨ä¼˜é€‰è¯åº“: {len(base_candidates)} ä¸ªå€™é€‰è¯")
                return base_candidates
        
        print("âš¡ åº”ç”¨é«˜æ•ˆé«˜çº§ç­›é€‰...")
        
        if hasattr(self, 'vocab_preprocessor') and self.vocab_preprocessor:
            return self._efficient_advanced_filter(query_word, character_filters)
        else:
            # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨åˆ†å±‚ç­›é€‰
            candidates = self._get_candidate_pool(query_word)
            filtered_words = self._apply_layered_filtering(candidates, character_filters)
            print(f"ğŸ“š å¤‡ç”¨ç­›é€‰å®Œæˆ: {len(filtered_words)} ä¸ªå€™é€‰è¯")
            return filtered_words
    
    def _efficient_advanced_filter(self, query_word: str, character_filters: List[Dict[str, Any]]) -> List[str]:
        """
        é«˜æ•ˆçš„é«˜çº§ç­›é€‰ç®—æ³•ï¼š
        1. äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯ï¼ˆåŸºäºå£°æ¯/éŸµæ¯ï¼‰
        2. é€è¯éªŒè¯æ‰€æœ‰æ¡ä»¶ï¼ˆå£°è°ƒã€ç¬”ç”»ã€éƒ¨é¦–ç­‰ï¼‰
        """
        print(f"   é«˜çº§ç­›é€‰æ¡ä»¶: {len(character_filters)} ä¸ªå­—ç¬¦ä½ç½®")
        
        # æ­¥éª¤1: ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯é›†åˆ
        initial_candidates = self._get_candidates_by_advanced_binary_search(character_filters)
        
        if not initial_candidates:
            print("   äºŒåˆ†æŸ¥æ‰¾æ— ç»“æœ")
            return []
        
        print(f"   äºŒåˆ†æŸ¥æ‰¾ç»“æœ: {len(initial_candidates)} ä¸ªå€™é€‰è¯")
        
        # æ­¥éª¤2: é€è¯éªŒè¯æ‰€æœ‰é«˜çº§æ¡ä»¶
        verified_candidates = self._verify_advanced_conditions(initial_candidates, character_filters)
        
        # ç§»é™¤æŸ¥è¯¢è¯æœ¬èº«
        verified_candidates = [w for w in verified_candidates if w != query_word]
        
        print(f"   é«˜çº§éªŒè¯å®Œæˆ: {len(verified_candidates)} ä¸ªç¬¦åˆæ¡ä»¶çš„è¯")
        return verified_candidates
    
    def _get_candidates_by_advanced_binary_search(self, character_filters: List[Dict[str, Any]]) -> List[str]:
        """ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾è·å–åˆå§‹å€™é€‰è¯é›†åˆï¼ˆåŸºäºå£°æ¯/éŸµæ¯æ¡ä»¶ï¼‰"""
        
        # å¯»æ‰¾æœ€å®¹æ˜“ç”¨ç´¢å¼•æŸ¥æ‰¾çš„æ¡ä»¶ï¼ˆå£°æ¯æˆ–éŸµæ¯ï¼‰
        best_filter = None
        best_position = -1
        best_type = None
        
        for i, filters in enumerate(character_filters):
            if not filters:
                continue
            
            # ä¼˜å…ˆä½¿ç”¨éŸµæ¯ï¼ˆé€šå¸¸æ›´å…·é€‰æ‹©æ€§ï¼‰
            if filters.get('final'):
                best_filter = filters['final']
                best_position = i
                best_type = 'final'
                break
            # å…¶æ¬¡ä½¿ç”¨å£°æ¯
            elif filters.get('initial'):
                best_filter = filters['initial']
                best_position = i
                best_type = 'initial'
        
        if not best_filter:
            # æ²¡æœ‰å£°æ¯/éŸµæ¯æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰è¯æ±‡
            print("   æ— å£°æ¯/éŸµæ¯æ¡ä»¶ï¼Œä½¿ç”¨å…¨è¯åº“")
            return self.vocab_preprocessor.get_all_words()
        
        print(f"   ä½¿ç”¨ç¬¬{best_position+1}ä¸ªå­—çš„{best_type}'{best_filter}'è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾")
        
        # ä½¿ç”¨ç´¢å¼•å¿«é€Ÿè·å–å€™é€‰è¯
        if best_type == 'final':
            candidates = self.vocab_preprocessor.filter_words_by_final_fast(best_filter)
        else:  # initial
            # å¤„ç†å£°æ¯å­—ç¬¦è½¬æ¢ï¼ˆg -> É¡ï¼‰
            search_initial = best_filter
            if search_initial == 'g':
                search_initial = 'É¡'  # Unicode U+0261
            candidates = self.vocab_preprocessor.filter_words_by_initial_fast(search_initial)
        
        print(f"   äºŒåˆ†æŸ¥æ‰¾ç»“æœ: {len(candidates)} ä¸ªå€™é€‰è¯")
        return candidates
    
    def _verify_advanced_conditions(self, candidates: List[str], character_filters: List[Dict[str, Any]]) -> List[str]:
        """é€è¯éªŒè¯å€™é€‰è¯æ˜¯å¦æ»¡è¶³æ‰€æœ‰é«˜çº§æ¡ä»¶"""
        
        verified = []
        
        # è®¡ç®—å®é™…æœ‰æ•ˆçš„ç­›é€‰æ¡ä»¶æ•°é‡ï¼ˆå»é™¤ç©ºæ¡ä»¶ï¼‰
        effective_filters = [f for f in character_filters if f]
        max_position = 0
        for i, f in enumerate(character_filters):
            if f:
                max_position = i + 1
        
        for word in candidates:
            # è¯é•¿å¿…é¡»è‡³å°‘åŒ…å«æœ‰æ¡ä»¶çš„ä½ç½®
            if len(word) < max_position:
                continue
            
            # éªŒè¯æ¯ä¸ªå­—ç¬¦ä½ç½®çš„æ¡ä»¶
            if self._verify_word_conditions(word, character_filters):
                verified.append(word)
        
        return verified
    
    def _filter_by_length(self, candidates: List[str], min_length: Optional[int], max_length: Optional[int]) -> List[str]:
        """æŒ‰è¯æ±‡é•¿åº¦ç­›é€‰å€™é€‰è¯"""
        if min_length is None and max_length is None:
            return candidates
        
        filtered = []
        for word in candidates:
            word_length = len(word)
            # æ£€æŸ¥æœ€å°é•¿åº¦
            if min_length is not None and word_length < min_length:
                continue
            # æ£€æŸ¥æœ€å¤§é•¿åº¦
            if max_length is not None and word_length > max_length:
                continue
            filtered.append(word)
        
        return filtered
    
    def _verify_word_conditions(self, word: str, character_filters: List[Dict[str, Any]]) -> bool:
        """éªŒè¯å•ä¸ªè¯æ±‡æ˜¯å¦æ»¡è¶³æ‰€æœ‰æ¡ä»¶"""
        
        # è·å–è¯æ±‡æ‹¼éŸ³
        pinyins = self.vocab_preprocessor.get_word_pinyin(word)
        if not pinyins:
            return False
        
        # å¤„ç†æ‹¼éŸ³æ ¼å¼ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²è¡¨ç¤ºçš„åˆ—è¡¨ï¼Œéœ€è¦è§£æ
        pinyin_string = pinyins[0]
        if isinstance(pinyin_string, str) and pinyin_string.startswith('[') and pinyin_string.endswith(']'):
            # è§£æå­—ç¬¦ä¸²è¡¨ç¤ºçš„åˆ—è¡¨ï¼Œå¦‚"['xuÃ©']" -> "xuÃ©"
            import ast
            try:
                pinyin_list = ast.literal_eval(pinyin_string)
                if isinstance(pinyin_list, list) and len(pinyin_list) > 0:
                    pinyin_string = ' '.join(pinyin_list)
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•çš„å­—ç¬¦ä¸²å¤„ç†
                pinyin_string = pinyin_string.strip("[]'\"").replace("'", "").replace('"', '')
        
        pinyin_parts = pinyin_string.split()
        
        # é€å­—ç¬¦éªŒè¯ï¼ˆåªéªŒè¯æœ‰æ¡ä»¶çš„ä½ç½®ï¼‰
        for i, filters in enumerate(character_filters):
            if not filters:  # ç©ºæ¡ä»¶ï¼Œè·³è¿‡
                continue
                
            # æ£€æŸ¥è¯æ±‡æ˜¯å¦æœ‰è¶³å¤Ÿçš„å­—ç¬¦
            if i >= len(word) or i >= len(pinyin_parts):
                return False
            
            char = word[i]
            char_pinyin = pinyin_parts[i]
            
            # éªŒè¯æ‹¼éŸ³ç›¸å…³æ¡ä»¶ï¼ˆå£°æ¯ã€éŸµæ¯ã€å£°è°ƒï¼‰
            if not self._verify_pinyin_conditions(char_pinyin, filters):
                return False
            
            # éªŒè¯å­—ç¬¦ç›¸å…³æ¡ä»¶ï¼ˆç¬”ç”»ã€éƒ¨é¦–ç­‰ï¼‰
            if not self._verify_character_conditions(char, filters):
                return False
        
        return True
    
    def _verify_pinyin_conditions(self, char_pinyin: str, filters: Dict[str, Any]) -> bool:
        """éªŒè¯æ‹¼éŸ³ç›¸å…³æ¡ä»¶ï¼ˆç©ºå€¼è¡¨ç¤ºæ— é™åˆ¶ï¼‰"""
        
        # æå–å£°æ¯å’ŒéŸµæ¯
        initial, final = self.vocab_preprocessor._extract_initial_final(char_pinyin)
        
        # éªŒè¯å£°æ¯ï¼ˆç©ºå€¼æˆ–Noneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_initial = filters.get('initial')
        if required_initial and required_initial.strip():  # æœ‰å…·ä½“è¦æ±‚
            # å¤„ç†å£°æ¯å­—ç¬¦è½¬æ¢ï¼ˆg -> É¡ï¼‰
            search_initial = required_initial.strip()
            if search_initial == 'g':
                search_initial = 'É¡'  # Unicode U+0261
            if initial != search_initial:
                return False
        
        # éªŒè¯éŸµæ¯ï¼ˆç©ºå€¼æˆ–Noneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_final = filters.get('final')
        if required_final and required_final.strip():  # æœ‰å…·ä½“è¦æ±‚
            # æ”¯æŒéŸµæ¯æ ‡å‡†åŒ–æ¯”è¾ƒ
            def normalize_final(f):
                if not f:
                    return ''
                # å¤„ç†Unicode É¡å­—ç¬¦
                normalized = f.replace('É¡', 'g')
                # å¤„ç†ue <-> veè½¬æ¢ï¼šç»Ÿä¸€ä¸ºueæ ‡å‡†
                if normalized == 've':
                    normalized = 'ue'
                return normalized
            
            actual_normalized = normalize_final(final)
            required_normalized = normalize_final(required_final.strip())
            
            # æ”¯æŒåŒå‘åŒ¹é…ï¼šueå¯ä»¥åŒ¹é…veï¼Œveä¹Ÿå¯ä»¥åŒ¹é…ue
            if actual_normalized != required_normalized and not (
                (actual_normalized == 'ue' and required_normalized == 've') or
                (actual_normalized == 've' and required_normalized == 'ue')
            ):
                return False
        
        # éªŒè¯å£°è°ƒï¼ˆéœ€è¦ä»åŸå§‹æ‹¼éŸ³æå–ï¼‰
        required_tone = filters.get('tone')
        if required_tone and required_tone.strip():  # æœ‰å…·ä½“è¦æ±‚
            # ç®€å•çš„å£°è°ƒæå–ï¼ˆåŸºäºéŸ³è°ƒç¬¦å·ï¼‰
            tone = self._extract_tone(char_pinyin)
            if tone != filters['tone']:
                return False
        
        return True
    
    def _verify_character_conditions(self, char: str, filters: Dict[str, Any]) -> bool:
        """éªŒè¯å­—ç¬¦ç›¸å…³æ¡ä»¶ï¼ˆç¬”ç”»ã€éƒ¨é¦–ç­‰ï¼Œç©ºå€¼è¡¨ç¤ºæ— é™åˆ¶ï¼‰"""
        
        # è·å–å­—ç¬¦ä¿¡æ¯
        char_info = self.vocab_preprocessor.get_character_info(char)
        if not char_info:
            # å¦‚æœæ²¡æœ‰å­—ç¬¦ä¿¡æ¯ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¿…é¡»éªŒè¯çš„æ¡ä»¶
            has_required_conditions = any([
                filters.get('stroke_count'),
                filters.get('radical') and filters.get('radical').strip(),
                filters.get('contains_stroke') and filters.get('contains_stroke').strip()
            ])
            return not has_required_conditions
        
        # éªŒè¯ç¬”ç”»æ•°ï¼ˆ0æˆ–Noneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_stroke = filters.get('stroke_count')
        if required_stroke and required_stroke > 0:  # æœ‰å…·ä½“è¦æ±‚
            actual_stroke = char_info.get('stroke', 0)
            if actual_stroke != required_stroke:
                return False
        
        # éªŒè¯éƒ¨é¦–ï¼ˆç©ºå€¼æˆ–Noneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_radical = filters.get('radical')
        if required_radical and required_radical.strip():  # æœ‰å…·ä½“è¦æ±‚
            actual_radical = char_info.get('radical', '')
            if actual_radical != required_radical.strip():
                return False
        
        # éªŒè¯ç¬”ç”»ç›¸å…³æ¡ä»¶ï¼ˆç©ºå€¼è¡¨ç¤ºæ— é™åˆ¶ï¼‰
        required_stroke_char = filters.get('contains_stroke')
        if required_stroke_char and required_stroke_char.strip():  # æœ‰å…·ä½“è¦æ±‚
            order_simple = char_info.get('order_simple', [])
            if not self._verify_stroke_conditions(order_simple, filters):
                return False
        
        return True
    
    def _extract_tone(self, pinyin) -> str:
        """ä»æ‹¼éŸ³ä¸­æå–å£°è°ƒ"""
        # å£°è°ƒç¬¦å·æ˜ å°„
        tone_map = {
            'Ä': '1', 'Ã¡': '2', 'Ç': '3', 'Ã ': '4',
            'Ä“': '1', 'Ã©': '2', 'Ä›': '3', 'Ã¨': '4',
            'Ä«': '1', 'Ã­': '2', 'Ç': '3', 'Ã¬': '4',
            'Å': '1', 'Ã³': '2', 'Ç’': '3', 'Ã²': '4',
            'Å«': '1', 'Ãº': '2', 'Ç”': '3', 'Ã¹': '4',
            'Ç–': '1', 'Ç˜': '2', 'Çš': '3', 'Çœ': '4',
            'Å„': '2', 'Åˆ': '3', 'Ç¹': '4',
            'á¸¿': '2', 'Åˆ': '3', 'Ç¹': '4'
        }
        
        # å¤„ç†ä¸åŒçš„æ‹¼éŸ³æ ¼å¼
        if isinstance(pinyin, list) and pinyin:
            clean_pinyin = pinyin[0]  # å–ç¬¬ä¸€ä¸ªæ‹¼éŸ³
        elif isinstance(pinyin, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²è¡¨ç¤ºçš„åˆ—è¡¨æ ¼å¼ï¼Œå¦‚ "['xuÃ©']"
            if pinyin.startswith('[') and pinyin.endswith(']'):
                # è§£æå­—ç¬¦ä¸²è¡¨ç¤ºçš„åˆ—è¡¨
                import ast
                try:
                    parsed = ast.literal_eval(pinyin)
                    if isinstance(parsed, list) and parsed:
                        clean_pinyin = parsed[0]
                    else:
                        clean_pinyin = pinyin.strip("[]'\"")
                except:
                    clean_pinyin = pinyin.strip("[]'\"")
            else:
                clean_pinyin = pinyin.strip()
        else:
            return '5'
        
        # æ¸…ç†æ‹¼éŸ³å­—ç¬¦ä¸²
        clean_pinyin = str(clean_pinyin).strip()
        
        for char in clean_pinyin:
            if char in tone_map:
                return tone_map[char]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å£°è°ƒç¬¦å·ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•°å­—å£°è°ƒï¼ˆå¦‚xue2ï¼‰
        import re
        tone_match = re.search(r'(\d)$', clean_pinyin)
        if tone_match:
            return tone_match.group(1)
        
        return '5'  # è½»å£°æˆ–æ— å£°è°ƒ
    
    def _verify_stroke_conditions(self, order_simple: List[str], filters: Dict[str, Any]) -> bool:
        """éªŒè¯ç¬”ç”»ç›¸å…³æ¡ä»¶"""
        
        contains_stroke = filters.get('contains_stroke')
        stroke_position = filters.get('stroke_position')
        
        if contains_stroke:
            if stroke_position:
                # éªŒè¯ç‰¹å®šä½ç½®çš„ç¬”ç”»
                pos = int(stroke_position) - 1  # è½¬æ¢ä¸º0ç´¢å¼•
                if pos < 0 or pos >= len(order_simple) or order_simple[pos] != contains_stroke:
                    return False
            else:
                # éªŒè¯æ˜¯å¦åŒ…å«æŸç§ç¬”ç”»
                if contains_stroke not in order_simple:
                    return False
        
        return True
    
    def _apply_layered_filtering(self, candidates: List[str], character_filters: List[Dict[str, Any]]) -> List[str]:
        """åˆ†å±‚ç­›é€‰ç­–ç•¥ï¼šé¢„ç­›é€‰ â†’ é€æ­¥ç²¾åŒ– â†’ æœ€ç»ˆä¼˜åŒ–"""
        
        # ç¬¬ä¸€å±‚ï¼šé¢„ç­›é€‰ - ä½¿ç”¨æœ€ä¸¥æ ¼çš„æ¡ä»¶å¿«é€Ÿè¿‡æ»¤
        print("ğŸ” ç¬¬ä¸€å±‚ï¼šé¢„ç­›é€‰é˜¶æ®µ...")
        filtered_candidates = self._pre_filter_candidates(candidates, character_filters)
        
        if not filtered_candidates:
            print("âŒ é¢„ç­›é€‰åæ— ç»“æœ")
            return []
        
        print(f"   é¢„ç­›é€‰å®Œæˆ: {len(candidates)} â†’ {len(filtered_candidates)} ä¸ªå€™é€‰è¯")
        
        # ç¬¬äºŒå±‚ï¼šæ¸è¿›å¼ç­›é€‰ - é€ä¸ªä½ç½®åº”ç”¨ç­›é€‰æ¡ä»¶
        print("ğŸ¯ ç¬¬äºŒå±‚ï¼šæ¸è¿›å¼ç­›é€‰...")
        final_candidates = self._progressive_filtering(filtered_candidates, character_filters)
        
        if not final_candidates:
            print("âŒ æ¸è¿›å¼ç­›é€‰åæ— ç»“æœ")
            return []
        
        print(f"   æ¸è¿›å¼ç­›é€‰å®Œæˆ: {len(filtered_candidates)} â†’ {len(final_candidates)} ä¸ªå€™é€‰è¯")
        
        return final_candidates
    
    def _pre_filter_candidates(self, candidates: List[str], character_filters: List[Dict[str, Any]]) -> List[str]:
        """é¢„ç­›é€‰ï¼šä½¿ç”¨æœ€ä¸¥æ ¼çš„æ¡ä»¶å¿«é€Ÿè¿‡æ»¤"""
        
        # ç»Ÿè®¡ç­›é€‰æ¡ä»¶çš„ä¸¥æ ¼ç¨‹åº¦
        strictness_scores = []
        for position, filters in enumerate(character_filters):
            if not filters:
                continue
            
            score = 0
            # å£°è°ƒå’Œå£°æ¯/éŸµæ¯ç»„åˆæœ€ä¸¥æ ¼
            if filters.get('tone') and (filters.get('initial') or filters.get('final')):
                score += 10
            # ç¬”ç”»æ•° + éƒ¨é¦–ç»„åˆä¸¥æ ¼
            elif filters.get('stroke_count') and filters.get('radical'):
                score += 8
            # ç‰¹å®šä½ç½®ç¬”ç”»å¾ˆä¸¥æ ¼
            elif filters.get('stroke_position') and filters.get('contains_stroke'):
                score += 7
            # å•ä¸€æ¡ä»¶
            elif any(filters.values()):
                score += 3
            
            strictness_scores.append((position, score, filters))
        
        # æŒ‰ä¸¥æ ¼ç¨‹åº¦æ’åºï¼Œä¼˜å…ˆä½¿ç”¨æœ€ä¸¥æ ¼çš„æ¡ä»¶
        strictness_scores.sort(key=lambda x: x[1], reverse=True)
        
        if not strictness_scores:
            return candidates
        
        # ä½¿ç”¨æœ€ä¸¥æ ¼çš„æ¡ä»¶è¿›è¡Œé¢„ç­›é€‰
        position, score, filters = strictness_scores[0]
        print(f"   ä½¿ç”¨ç¬¬{position+1}ä¸ªå­—çš„æ¡ä»¶è¿›è¡Œé¢„ç­›é€‰ (ä¸¥æ ¼åº¦: {score})")
        
        filtered = filter_words_by_advanced_criteria(
            candidates,
            character_position=position,
            initial=filters.get('initial'),
            final=filters.get('final'), 
            tone=filters.get('tone'),
            stroke_count=filters.get('stroke_count'),
            radical=filters.get('radical'),
            contains_stroke=filters.get('contains_stroke'),
            stroke_position=filters.get('stroke_position')
        )
        
        return filtered
    
    def _progressive_filtering(self, candidates: List[str], character_filters: List[Dict[str, Any]]) -> List[str]:
        """æ¸è¿›å¼ç­›é€‰ï¼šé€ä¸ªä½ç½®åº”ç”¨å‰©ä½™çš„ç­›é€‰æ¡ä»¶"""
        
        filtered_words = candidates
        
        for position, filters in enumerate(character_filters):
            if not filters:
                continue
            
            print(f"     ç­›é€‰ç¬¬{position+1}ä¸ªå­—: {filters}")
            
            before_count = len(filtered_words)
            filtered_words = filter_words_by_advanced_criteria(
                filtered_words,
                character_position=position,
                initial=filters.get('initial'),
                final=filters.get('final'), 
                tone=filters.get('tone'),
                stroke_count=filters.get('stroke_count'),
                radical=filters.get('radical'),
                contains_stroke=filters.get('contains_stroke'),
                stroke_position=filters.get('stroke_position')
            )
            
            after_count = len(filtered_words)
            print(f"     ç¬¬{position+1}ä¸ªå­—ç­›é€‰: {before_count} â†’ {after_count} ä¸ªå€™é€‰è¯")
            
            # å¦‚æœç­›é€‰åæ²¡æœ‰ç»“æœï¼Œæå‰é€€å‡º
            if not filtered_words:
                print(f"     âŒ ç¬¬{position+1}ä¸ªå­—ç­›é€‰åæ— ç»“æœï¼Œåœæ­¢")
                break
        
        return filtered_words
    
    def _pure_filter_search(self, character_filters: List[Dict[str, Any]], k: int) -> Tuple[List[str], List[float], str]:
        """
        çº¯ç­›é€‰æœç´¢æ¨¡å¼ - ä¸ä¾èµ–embeddingï¼Œåªæ ¹æ®ç­›é€‰æ¡ä»¶ç­›é€‰è¯æ±‡
        """
        print("ğŸ” çº¯ç­›é€‰æ¨¡å¼ï¼šåŸºäºç­›é€‰æ¡ä»¶æœç´¢è¯æ±‡...")
        
        try:
            # è·å–å€™é€‰è¯æ± ï¼Œä¼˜å…ˆçº§ï¼šå¤§è¯åº“ > åŸºç¡€è¯åº“ > ç¦»çº¿ç­›é€‰å™¨
            candidates = None
            source_name = ""
            
            if self.large_vocabulary and len(self.large_vocabulary) > 1000:
                candidates = self.large_vocabulary
                source_name = f"å¤§è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯"
            elif self.candidate_words and len(self.candidate_words) > 0:
                candidates = self.candidate_words
                source_name = f"åŸºç¡€è¯åº“: {len(candidates)} ä¸ªå€™é€‰è¯"
            elif self.offline_filter:
                # ä½¿ç”¨ç¦»çº¿ç­›é€‰å™¨
                print("ğŸ“– ä½¿ç”¨ç¦»çº¿ç­›é€‰å™¨")
                return self.offline_filter.pure_filter_search(character_filters, k)
            else:
                return [], [], "âŒ æ²¡æœ‰å¯ç”¨çš„è¯åº“æ•°æ®"
            
            print(f"ğŸ“– ä½¿ç”¨{source_name}")
            
            # åº”ç”¨åˆ†å±‚ç­›é€‰
            filtered_words = self._apply_layered_filtering(candidates, character_filters)
            
            if not filtered_words:
                return [], [], "ğŸ¯ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ‰€æœ‰ç­›é€‰æ¡ä»¶çš„è¯æ±‡"
            
            # é™åˆ¶ç»“æœæ•°é‡
            final_results = filtered_words[:k] if len(filtered_words) > k else filtered_words
            
            # ç”Ÿæˆé›¶ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå› ä¸ºæ²¡æœ‰å‚è€ƒè¯æ±‡ï¼‰
            zero_similarities = [0.0] * len(final_results)
            
            # æ„å»ºç»“æœæ¶ˆæ¯
            result_msg = self._build_pure_filter_result_message(final_results, character_filters, len(filtered_words))
            
            print(f"âœ… çº¯ç­›é€‰å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results, zero_similarities, result_msg
            
        except Exception as e:
            error_msg = f"âŒ çº¯ç­›é€‰æœç´¢å¤±è´¥: {str(e)}"
            print(error_msg)
            return [], [], error_msg
    
    def _build_pure_filter_result_message(self, results: List[str], character_filters: List[Dict[str, Any]], total_matches: int) -> str:
        """æ„å»ºçº¯ç­›é€‰ç»“æœæ¶ˆæ¯"""
        
        result_lines = ["ğŸ” çº¯ç­›é€‰æœç´¢ç»“æœ"]
        result_lines.append(f"ğŸ“Š æœç´¢æ¨¡å¼: ä»…åŸºäºç­›é€‰æ¡ä»¶ï¼ˆæ— embeddingè®¡ç®—ï¼‰")
        
        # ç­›é€‰æ¡ä»¶ä¿¡æ¯
        if character_filters:
            condition_info = []
            for i, filters in enumerate(character_filters):
                if not filters:
                    continue
                
                char_conditions = []
                if filters.get('initial'):
                    char_conditions.append(f"å£°æ¯={filters['initial']}")
                if filters.get('final'):
                    char_conditions.append(f"éŸµæ¯={filters['final']}")
                if filters.get('tone'):
                    char_conditions.append(f"å£°è°ƒ={filters['tone']}")
                if filters.get('stroke_count'):
                    char_conditions.append(f"ç¬”ç”»={filters['stroke_count']}")
                if filters.get('radical'):
                    char_conditions.append(f"éƒ¨é¦–={filters['radical']}")
                if filters.get('contains_stroke'):
                    if filters.get('stroke_position'):
                        char_conditions.append(f"ç¬¬{filters['stroke_position']}ç¬”={filters['contains_stroke']}")
                    else:
                        char_conditions.append(f"å«ç¬”ç”»={filters['contains_stroke']}")
                
                if char_conditions:
                    condition_info.append(f"ç¬¬{i+1}å­—({', '.join(char_conditions)})")
            
            if condition_info:
                result_lines.append(f"ğŸ¯ ç­›é€‰æ¡ä»¶: {'; '.join(condition_info)}")
        
        result_lines.append(f"ğŸ“ˆ åŒ¹é…ç»Ÿè®¡: å…±æ‰¾åˆ° {total_matches} ä¸ªç¬¦åˆæ¡ä»¶çš„è¯æ±‡")
        result_lines.append(f"ğŸ“ æ˜¾ç¤ºç»“æœ: å‰ {len(results)} ä¸ªè¯æ±‡")
        result_lines.append("")
        
        # ç»“æœåˆ—è¡¨
        for i, word in enumerate(results, 1):
            result_lines.append(f"{i:2d}. {word} (ç­›é€‰åŒ¹é…)")
        
        return "\n".join(result_lines)
    
    def _compute_similarities(self, query_word: str, candidates: List[str], k: int) -> Tuple[List[str], List[float]]:
        """è®¡ç®—æŸ¥è¯¢è¯ä¸å€™é€‰è¯çš„ç›¸ä¼¼åº¦ï¼ˆé«˜æ•ˆç‰ˆï¼‰"""
        
        if not candidates:
            return [], []
        
        try:
            # é™åˆ¶å€™é€‰è¯æ•°é‡ä»¥æé«˜æ•ˆç‡
            max_candidates = 1000  # æœ€å¤§å¤„ç†å€™é€‰è¯æ•°é‡
            if len(candidates) > max_candidates:
                print(f"ğŸ“Š å€™é€‰è¯è¿‡å¤š ({len(candidates)}ä¸ª)ï¼Œé™åˆ¶ä¸ºå‰{max_candidates}ä¸ª")
                candidates = candidates[:max_candidates]
            
            # å¦‚æœå€™é€‰è¯è¾ƒå¤šï¼Œåˆ†æ‰¹å¤„ç†
            max_batch_candidates = 300  # æ¯æ‰¹æœ€å¤§å€™é€‰è¯æ•°é‡
            
            if len(candidates) > max_batch_candidates:
                print(f"ğŸ“Š åˆ†æ‰¹å¤„ç† {len(candidates)} ä¸ªå€™é€‰è¯...")
                
                all_similarities = []
                batch_size = max_batch_candidates
                
                for i in range(0, len(candidates), batch_size):
                    batch_candidates = candidates[i:i + batch_size]
                    print(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(candidates) + batch_size - 1) // batch_size}")
                    
                    batch_similarities = self._compute_batch_similarities(query_word, batch_candidates)
                    all_similarities.extend(batch_similarities)
                
                # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
                all_similarities.sort(key=lambda x: x[1], reverse=True)
                top_k = all_similarities[:k]
                
                synonyms = [pair[0] for pair in top_k]
                sim_scores = [pair[1] for pair in top_k]
                
                return synonyms, sim_scores
            else:
                # å€™é€‰è¯è¾ƒå°‘ï¼Œç›´æ¥å¤„ç†
                print(f"ğŸ§  ç›´æ¥è®¡ç®— {len(candidates)} ä¸ªå€™é€‰è¯çš„ç›¸ä¼¼åº¦")
                return self._compute_batch_similarities_with_topk(query_word, candidates, k)
            
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return [], []
    
    def _compute_batch_similarities(self, query_word: str, candidates: List[str]) -> List[Tuple[str, float]]:
        """è®¡ç®—ä¸€æ‰¹å€™é€‰è¯çš„ç›¸ä¼¼åº¦ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            # é™åˆ¶å•æ¬¡ç¼–ç çš„æœ€å¤§æ•°é‡ï¼ˆQwen APIé™åˆ¶ï¼‰
            max_batch_size = 30  # æŸ¥è¯¢è¯ + å€™é€‰è¯æ€»æ•°ä¸è¶…è¿‡32
            
            if len(candidates) > max_batch_size - 1:
                # å€™é€‰è¯å¤ªå¤šï¼Œåˆ†æ‰¹å¤„ç†
                return self._compute_batch_similarities_fallback(query_word, candidates)
            
            # å‡†å¤‡ç¼–ç è¯æ±‡åˆ—è¡¨ï¼ˆæŸ¥è¯¢è¯ + å€™é€‰è¯ï¼‰
            words_to_encode = [query_word] + candidates
            
            # æ‰¹é‡ç¼–ç 
            embeddings = self.qwen_client.encode(words_to_encode)
            
            if embeddings is None or len(embeddings) != len(words_to_encode):
                raise Exception("ç¼–ç å¤±è´¥æˆ–æ•°é‡ä¸åŒ¹é…")
            
            # åˆ†ç¦»æŸ¥è¯¢è¯å’Œå€™é€‰è¯çš„å‘é‡
            query_embedding = embeddings[0]
            candidate_embeddings = embeddings[1:]
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for i, candidate in enumerate(candidates):
                if i < len(candidate_embeddings):
                    candidate_embedding = candidate_embeddings[i]
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_embedding, candidate_embedding) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding)
                    )
                    similarities.append((candidate, float(similarity)))
            
            return similarities
            
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨åˆ†æ‰¹æ–¹æ³•
            return self._compute_batch_similarities_fallback(query_word, candidates)
    
    def _compute_batch_similarities_fallback(self, query_word: str, candidates: List[str]) -> List[Tuple[str, float]]:
        """å¤‡ç”¨çš„æ‰¹æ¬¡ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•"""
        # å‡†å¤‡ç¼–ç è¯æ±‡åˆ—è¡¨ï¼ˆæŸ¥è¯¢è¯ + å€™é€‰è¯ï¼‰
        words_to_encode = [query_word] + candidates
        
        # åˆ†æ‰¹ç¼–ç ä»¥é¿å…è¶…è¿‡APIé™åˆ¶
        batch_size = 20
        all_embeddings = []
        
        for i in range(0, len(words_to_encode), batch_size):
            batch = words_to_encode[i:i + batch_size]
            batch_embeddings = self.qwen_client.encode(batch)
            
            if batch_embeddings is None:
                print(f"âŒ æ‰¹æ¬¡ç¼–ç å¤±è´¥")
                continue
            
            all_embeddings.extend(batch_embeddings)
        
        if len(all_embeddings) != len(words_to_encode):
            print(f"âš ï¸ ç¼–ç æ•°é‡ä¸åŒ¹é…: {len(all_embeddings)} vs {len(words_to_encode)}")
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        query_embedding = all_embeddings[0]
        candidate_embeddings = all_embeddings[1:]
        
        similarities = []
        for i, candidate in enumerate(candidates):
            if i < len(candidate_embeddings):
                candidate_embedding = candidate_embeddings[i]
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(query_embedding, candidate_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding)
                )
                similarities.append((candidate, float(similarity)))
        
        return similarities
    
    def _compute_batch_similarities_with_topk(self, query_word: str, candidates: List[str], k: int) -> Tuple[List[str], List[float]]:
        """è®¡ç®—å€™é€‰è¯ç›¸ä¼¼åº¦å¹¶ç›´æ¥è¿”å›top-kç»“æœï¼ˆé¿å…é€’å½’è°ƒç”¨ï¼‰"""
        
        # ç›´æ¥ä½¿ç”¨fallbackæ–¹æ³•ï¼Œé¿å…é€’å½’è°ƒç”¨
        similarities = self._compute_batch_similarities_fallback(query_word, candidates)
        
        if not similarities:
            return [], []
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›top-kç»“æœ
        top_k = similarities[:k]
        synonyms = [pair[0] for pair in top_k]
        sim_scores = [pair[1] for pair in top_k]
        
        return synonyms, sim_scores
    
    def _build_unified_result_message(self, query_word: str, synonyms: List[str], similarities: List[float], 
                                    character_finals: Optional[List[str]], character_filters: Optional[List[Dict[str, Any]]], 
                                    sorted: bool = True, min_length: Optional[int] = None, max_length: Optional[int] = None) -> str:
        """æ„å»ºç»Ÿä¸€çš„ç»“æœæ¶ˆæ¯ï¼ˆæ”¯æŒç®€å•å’Œé«˜çº§ç­›é€‰ï¼‰"""
        
        if not synonyms:
            return "âŒ æœªæ‰¾åˆ°ç›¸ä¼¼è¯æ±‡"
        
        # åˆ¤æ–­æ˜¯å¦æœ‰æŸ¥è¯¢è¯
        has_query_word = bool(query_word and query_word.strip())
        
        # æ„å»ºç»“æœå±•ç¤º
        result_lines = []
        
        if has_query_word:
            result_lines.append(f"ğŸ” æŸ¥è¯¢è¯æ±‡: {query_word}")
        else:
            result_lines.append(f"ğŸ” çº¯ç­›é€‰æ¨¡å¼ï¼ˆæ— æŸ¥è¯¢è¯ï¼‰")
        
        if sorted and has_query_word:
            result_lines.append(f"ğŸ“Š æ•°æ®æ¥æº: Qwen3-Embedding-0.6B (1024ç»´å‘é‡)")
            result_lines.append(f"âš¡ ç®—æ³•: äºŒåˆ†æŸ¥æ‰¾ç­›é€‰ + è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº")
        else:
            result_lines.append(f"ğŸ“Š æ•°æ®æ¥æº: é¢„å¤„ç†è¯åº“ç´¢å¼•")
            result_lines.append(f"âš¡ ç®—æ³•: äºŒåˆ†æŸ¥æ‰¾ç­›é€‰ï¼ˆæœªæ’åºï¼‰")
        
        # ç­›é€‰æ¡ä»¶ä¿¡æ¯ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
        if character_filters:
            # é«˜çº§ç­›é€‰æ¨¡å¼
            filters_info = []
            for i, char_filter in enumerate(character_filters):
                if char_filter:
                    filter_parts = []
                    if char_filter.get('initial'):
                        filter_parts.append(f"å£°æ¯={char_filter['initial']}")
                    if char_filter.get('final'):
                        filter_parts.append(f"éŸµæ¯={char_filter['final']}")
                    if char_filter.get('tone'):
                        filter_parts.append(f"å£°è°ƒ={char_filter['tone']}")
                    if char_filter.get('stroke_count'):
                        filter_parts.append(f"ç¬”ç”»={char_filter['stroke_count']}")
                    if char_filter.get('radical'):
                        filter_parts.append(f"éƒ¨é¦–={char_filter['radical']}")
                    if char_filter.get('contains_stroke'):
                        if char_filter.get('stroke_position'):
                            filter_parts.append(f"ç¬¬{char_filter['stroke_position']}ç¬”={char_filter['contains_stroke']}")
                        else:
                            filter_parts.append(f"å«ç¬”ç”»={char_filter['contains_stroke']}")
                    
                    if filter_parts:
                        filters_info.append(f"ç¬¬{i+1}å­—({', '.join(filter_parts)})")
            
            if filters_info:
                result_lines.append(f"ğŸ¯ é«˜çº§ç­›é€‰: {', '.join(filters_info)}")
        
        elif character_finals and any(f for f in character_finals):
            # ç®€å•éŸµæ¯ç­›é€‰æ¨¡å¼
            finals_info = []
            for i, final in enumerate(character_finals):
                if final:
                    finals_info.append(f"ç¬¬{i+1}å­—='{final}'")
            if finals_info:
                result_lines.append(f"ğŸµ éŸµæ¯ç­›é€‰: {', '.join(finals_info)}")
        
        # é•¿åº¦ç­›é€‰ä¿¡æ¯
        if min_length is not None or max_length is not None:
            length_info = []
            if min_length is not None:
                length_info.append(f"æœ€å°é•¿åº¦={min_length}")
            if max_length is not None:
                length_info.append(f"æœ€å¤§é•¿åº¦={max_length}")
            result_lines.append(f"ğŸ“ é•¿åº¦ç­›é€‰: {', '.join(length_info)}")
        
        if sorted and has_query_word:
            result_lines.append(f"ğŸ“ æ‰¾åˆ° {len(synonyms)} ä¸ªè¿‘ä¹‰è¯ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰:")
        else:
            result_lines.append(f"ğŸ“ æ‰¾åˆ° {len(synonyms)} ä¸ªåŒ¹é…è¯ï¼ˆç­›é€‰ç»“æœï¼‰:")
        
        result_lines.append("")
        
        # è¯æ±‡åˆ—è¡¨
        for i, (synonym, similarity) in enumerate(zip(synonyms, similarities), 1):
            if sorted and has_query_word and similarity > 0:
                percentage = similarity * 100
                result_lines.append(f"{i:2d}. {synonym} ({percentage:.1f}%)")
            else:
                result_lines.append(f"{i:2d}. {synonym}")
        
        return "\n".join(result_lines)
    
    def _build_advanced_result_message(self, query_word: str, synonyms: List[str], similarities: List[float], character_filters: Optional[List[Dict[str, Any]]]) -> str:
        """æ„å»ºé«˜çº§æœç´¢ç»“æœæ¶ˆæ¯"""
        
        if not synonyms:
            return "âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç›¸ä¼¼è¯æ±‡"
        
        # æ„å»ºç»“æœå±•ç¤º
        result_lines = [f"ğŸ” æŸ¥è¯¢è¯æ±‡: {query_word}"]
        result_lines.append(f"ğŸ“Š æ•°æ®æ¥æº: Qwen3-Embedding-0.6B (1024ç»´å‘é‡)")
        result_lines.append(f"ğŸ¯ ç®—æ³•: è¯­ä¹‰ç›¸ä¼¼åº¦ + é«˜çº§æ±‰å­—ç­›é€‰")
        
        # æ£€æŸ¥æ˜¯å¦è¿›è¡Œäº†è¯­ä¹‰æ’åº
        semantic_count = sum(1 for s in similarities if s > 0)
        candidate_count = len(similarities) - semantic_count
        
        if semantic_count > 0 and candidate_count > 0:
            result_lines.append(f"ğŸ“ˆ ç»“æœç±»å‹: {semantic_count}ä¸ªè¯­ä¹‰æ’åºè¯ + {candidate_count}ä¸ªå€™é€‰è¯")
        elif semantic_count > 0:
            result_lines.append(f"ğŸ“ˆ ç»“æœç±»å‹: {semantic_count}ä¸ªè¯­ä¹‰æ’åºçš„åŒä¹‰è¯")
        elif candidate_count > 0:
            result_lines.append(f"ğŸ“ˆ ç»“æœç±»å‹: {candidate_count}ä¸ªå€™é€‰è¯ï¼ˆæœªè¿›è¡Œè¯­ä¹‰æ’åºï¼‰")
        
        # ç­›é€‰æ¡ä»¶ä¿¡æ¯
        if character_filters:
            condition_info = []
            for i, filters in enumerate(character_filters):
                if not filters:
                    continue
                
                char_conditions = []
                if filters.get('initial'):
                    char_conditions.append(f"å£°æ¯={filters['initial']}")
                if filters.get('final'):
                    char_conditions.append(f"éŸµæ¯={filters['final']}")
                if filters.get('tone'):
                    char_conditions.append(f"å£°è°ƒ={filters['tone']}")
                if filters.get('stroke_count'):
                    char_conditions.append(f"ç¬”ç”»={filters['stroke_count']}")
                if filters.get('radical'):
                    char_conditions.append(f"éƒ¨é¦–={filters['radical']}")
                if filters.get('contains_stroke'):
                    if filters.get('stroke_position'):
                        char_conditions.append(f"ç¬¬{filters['stroke_position']}ç¬”={filters['contains_stroke']}")
                    else:
                        char_conditions.append(f"å«ç¬”ç”»={filters['contains_stroke']}")
                
                if char_conditions:
                    condition_info.append(f"ç¬¬{i+1}å­—({', '.join(char_conditions)})")
            
            if condition_info:
                result_lines.append(f"ğŸ¯ ç­›é€‰æ¡ä»¶: {'; '.join(condition_info)}")
        
        result_lines.append(f"ğŸ“ æ‰¾åˆ° {len(synonyms)} ä¸ªç¬¦åˆæ¡ä»¶çš„è¿‘ä¹‰è¯:")
        result_lines.append("")
        
        # è¿‘ä¹‰è¯åˆ—è¡¨
        for i, (synonym, similarity) in enumerate(zip(synonyms, similarities), 1):
            if similarity > 0:
                # æœ‰è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
                percentage = similarity * 100
                result_lines.append(f"{i:2d}. {synonym} ({percentage:.1f}%)")
            else:
                # å€™é€‰è¯ï¼ˆæ— è¯­ä¹‰åˆ†æ•°ï¼‰
                result_lines.append(f"{i:2d}. {synonym} (å€™é€‰è¯)")
        
        return "\n".join(result_lines)

class QwenSynonymSearcherV2:
    """å…¼å®¹æ€§åŒ…è£…å™¨ - ä½¿ç”¨V3å®ç°"""
    
    def __init__(self, qwen_client=None):
        self.searcher_v3 = QwenSynonymSearcherV3(qwen_client)
        self.qwen_available = self.searcher_v3.qwen_available
    
    def search_synonyms(self, word: str, k: int = 10, character_finals: Optional[List[str]] = None) -> Tuple[List[str], List[float], str]:
        return self.searcher_v3.search_synonyms(word, k, character_finals)

# å¤„ç†å‡½æ•°
def process_qwen_synonym_search_v3(word: str, k: int, char1_final: str, char2_final: str, char3_final: str, char4_final: str) -> str:
    """å¤„ç†QwenåŒä¹‰è¯æŸ¥è¯¢V3"""
    try:
        searcher = QwenSynonymSearcherV3()
        
        # æ„å»ºéŸµæ¯ç­›é€‰æ¡ä»¶
        character_finals = [char1_final, char2_final, char3_final, char4_final]
        # ç§»é™¤æœ«å°¾çš„ç©ºå­—ç¬¦ä¸²
        while character_finals and not character_finals[-1]:
            character_finals.pop()
        
        synonyms, similarities, result_msg = searcher.search_synonyms(word, k, character_finals)
        return result_msg
        
    except Exception as e:
        return f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"


def process_qwen_synonym_search_advanced(
    word: str, 
    k: int,
    char1_conditions: Dict[str, Any] = None,
    char2_conditions: Dict[str, Any] = None,
    char3_conditions: Dict[str, Any] = None,
    char4_conditions: Dict[str, Any] = None
) -> str:
    """å¤„ç†Qwené«˜çº§åŒä¹‰è¯æŸ¥è¯¢"""
    try:
        searcher = QwenSynonymSearcherV3()
        
        # æ„å»ºç­›é€‰æ¡ä»¶
        character_filters = []
        for conditions in [char1_conditions, char2_conditions, char3_conditions, char4_conditions]:
            if conditions:
                # è¿‡æ»¤æ‰ç©ºå€¼
                filtered_conditions = {k: v for k, v in conditions.items() if v}
                character_filters.append(filtered_conditions)
            else:
                character_filters.append({})
        
        # ç§»é™¤æœ«å°¾çš„ç©ºæ¡ä»¶
        while character_filters and not character_filters[-1]:
            character_filters.pop()
        
        synonyms, similarities, result_msg = searcher.search_synonyms_advanced(word, k, character_filters)
        return result_msg
        
    except Exception as e:
        return f"âŒ é«˜çº§æŸ¥è¯¢å¤±è´¥: {str(e)}"


def get_available_search_options() -> Dict[str, List[str]]:
    """è·å–å¯ç”¨çš„æœç´¢é€‰é¡¹"""
    try:
        return {
            'initials': get_available_initials(),     # å£°æ¯
            'finals': get_available_finals(),         # éŸµæ¯
            'tones': get_available_tones(),           # å£°è°ƒ
            'strokes': get_available_strokes(),       # ç¬”ç”»
            'radicals': get_available_radicals()      # éƒ¨é¦–
        }
    except Exception as e:
        print(f"è·å–æœç´¢é€‰é¡¹å¤±è´¥: {e}")
        return {}

if __name__ == "__main__":
    # æµ‹è¯•
    searcher = QwenSynonymSearcherV3()
    result = searcher.search_synonyms("é«˜å…´", 5, ["ao", ""])
    print(result[2])
